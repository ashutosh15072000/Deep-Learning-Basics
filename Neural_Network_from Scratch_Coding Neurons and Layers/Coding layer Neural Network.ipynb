{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " `BUILDING NEURAL NETWORKS FROM SCRATCH PART 1: CODING NEURONS AND LAYERS`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>CODING OUR FIRST NEURON: 3 INPUTS</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](IMG/nueron_with_3_inputs.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3\n"
     ]
    }
   ],
   "source": [
    "inputs=[1,2,3]\n",
    "wieghts=[0.2,0.8,-0.5]\n",
    "bais=2\n",
    "\n",
    "output=(inputs[0]*wieghts[0]+inputs[1]*wieghts[1]+inputs[2]*wieghts[2]+bais)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "SINGLE NEURON USING NUMPY\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](IMG/single_neuron_with_numpy.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "Inputs=[1.0,2.0,3.0,2.5]\n",
    "Weights=[0.2,0.8,-0.5,1.0]\n",
    "bais=2.0\n",
    "output=np.dot(Inputs,Weights)+bais\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>CODING OUR SECOND NEURON: 4 INPUTS</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](IMG/nueron_with_4_inputs.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.8\n"
     ]
    }
   ],
   "source": [
    "inputs=[1.0,2.0,3.0,2.5]\n",
    "weights=[0.2,0.8,-0.5,1.0]\n",
    "bais=2.0\n",
    "output=(inputs[0]*weights[0]+\n",
    "        inputs[1]*weights[1]+\n",
    "        inputs[2]*weights[2]+\n",
    "        inputs[3]*weights[3]+\n",
    "        bais)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>CODING OUR FIRST LAYER</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](IMG/Layer_of_neuron.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8, 1.21, 2.385]\n"
     ]
    }
   ],
   "source": [
    "inputs = [1, 2, 3, 2.5]\n",
    "\n",
    "weights = [[0.2, 0.8, -0.5, 1],\n",
    " [0.5, -0.91, 0.26, -0.5],\n",
    " [-0.26, -0.27, 0.17, 0.87]]\n",
    "\n",
    "weights1 = weights[0] #LIST OF WEIGHTS ASSOCIATED WITH 1ST NEURON : W11, W12, W13, W14\n",
    "weights2 = weights[1] #LIST OF WEIGHTS ASSOCIATED WITH 2ND NEURON : W21, W22, W23, W24\n",
    "weights3 = weights[2] #LIST OF WEIGHTS ASSOCIATED WITH 3RD NEURON : W31, W32, W33, W34\n",
    "\n",
    "biases = [2, 3, 0.5]\n",
    "\n",
    "bias1 = 2\n",
    "bias2 = 3\n",
    "bias3 = 0.5\n",
    "\n",
    "outputs = [\n",
    " # Neuron 1:\n",
    " inputs[0]*weights1[0] +\n",
    " inputs[1]*weights1[1] +\n",
    " inputs[2]*weights1[2] +\n",
    " inputs[3]*weights1[3] + bias1,\n",
    " # Neuron 2:\n",
    " inputs[0]*weights2[0] +\n",
    " inputs[1]*weights2[1] +\n",
    " inputs[2]*weights2[2] +\n",
    " inputs[3]*weights2[3] + bias2,\n",
    " # Neuron 3:\n",
    " inputs[0]*weights3[0] +\n",
    " inputs[1]*weights3[1] +\n",
    " inputs[2]*weights3[2] +\n",
    " inputs[3]*weights3[3] + bias3]\n",
    "\n",
    "print(outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "USING LOOPS FOR BETTER AND EASIER CODING</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8, 6.01, 8.395]\n"
     ]
    }
   ],
   "source": [
    "inputs = [1, 2, 3, 2.5]\n",
    "\n",
    "##LIST OF WEIGHTS\n",
    "weights = [[0.2, 0.8, -0.5, 1],\n",
    " [0.5, -0.91, 0.26, -0.5],\n",
    " [-0.26, -0.27, 0.17, 0.87]]\n",
    "\n",
    "##LIST OF BIASES\n",
    "biases = [2, 3, 0.5]\n",
    "\n",
    "# Output of current layer\n",
    "neuron_output = 0\n",
    "layer_outputs=[]\n",
    "# For each neuron\n",
    "for neuron_weights, neuron_bias in zip(weights, biases):\n",
    "    for weight,n_inputs in zip(neuron_weights,inputs):\n",
    "         # Multiply this input by associated weight\n",
    "         # and add to the neuron's output variable\n",
    "        neuron_output += n_inputs*weight ## W31*X1 + W32*X2 + W33*X3 + W34*X4\n",
    "   # Add bias\n",
    "    neuron_output += neuron_bias ## ## W31*X1 + W32*X2 + W33*X3 + W34*X4 + B3\n",
    "\n",
    "    layer_outputs.append(neuron_output)    \n",
    "print(layer_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "LAYER OF NEURONS USING NUMPY\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](<IMG/Screenshot 2024-10-25 152610.jpg>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8   1.21  2.385]\n"
     ]
    }
   ],
   "source": [
    "inputs = [1.0, 2.0, 3.0, 2.5]\n",
    "weights = [[0.2, 0.8, -0.5, 1],\n",
    " [0.5, -0.91, 0.26, -0.5],\n",
    " [-0.26, -0.27, 0.17, 0.87]]\n",
    "biases = [2.0, 3.0, 0.5]\n",
    "\n",
    "# A dot product of a matrix and a vector results in a list of dot products. \n",
    "#The np.dot() method treats the matrix as a list of vectors and performs a dot product of each of those vectors with the other vector\n",
    "layer_outputs = np.dot(weights, inputs) + biases\n",
    "print(layer_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "LAYER OF NEURONS AND BATCH OF DATA USING NUMPY\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](IMG/Batch_of_inputs.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "NEED TO TAKE TRANSPOSE OF WEIGHT MATRIX</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](IMG/transpose_of.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.8    1.21   2.385]\n",
      " [ 8.9   -1.81   0.2  ]\n",
      " [ 1.41   1.051  0.026]]\n"
     ]
    }
   ],
   "source": [
    "## Batch of Inputs\n",
    "inputs=[[1.0,2.0,3.0,2.5],\n",
    "        [2.0,5.0,-1.0,2.0],\n",
    "        [-1.5,2.7,3.3,-0.8]]\n",
    "weights=[[0.2,0.8,-0.5,1],\n",
    "         [0.5,-0.91,0.26,-0.5],\n",
    "         [-0.26,-0.27,0.17,0.87]]\n",
    "bais=[2.0,3.0,0.5]\n",
    "# NOTE: We cant Transpose Lists in python so we have the convert the weights matrix into array first\n",
    "outputs=np.dot(inputs,np.array(weights).T)+bais\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "2 LAYERS AND BATCH OF DATA USING NUMPY\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](IMG/Two_layers.jpg)\n",
    "![alt text](IMG/weight_of_two_layer.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5031  -1.04185 -2.03875]\n",
      " [ 0.2434  -2.7332  -5.7633 ]\n",
      " [-0.99314  1.41254 -0.35655]]\n"
     ]
    }
   ],
   "source": [
    "## Batch of Inputs\n",
    "inputs=[[1.0,2.0,3.0,2.5],\n",
    "        [2.0,5.0,-1.0,2.0],\n",
    "        [-1.5,2.7,3.3,-0.8]]\n",
    "## First Layers Weights\n",
    "weights=[[0.2,0.8,-0.5,1],\n",
    "         [0.5,-0.91,0.26,-0.5],\n",
    "         [-0.26,-0.27,0.17,0.87]]\n",
    "\n",
    "baise1=[2,3,0.5]\n",
    "\n",
    "## Second Layers Weights\n",
    "weights2=[[0.1,-0.14,0.5],\n",
    "          [-0.5,0.12,-0.33],\n",
    "          [-0.44,0.73,-0.13]\n",
    "        ]\n",
    "bais2=[-1,2,-0.5]\n",
    "\n",
    "## Convert lists to numpy array\n",
    "inputs_array=np.array(inputs)\n",
    "weights_array=np.array(weights)\n",
    "bais_array=np.array(baise1)\n",
    "weight2_array=np.array(weights2)\n",
    "baises2_array=np.array(bais2)\n",
    "\n",
    "## CALCULATE THE OUTPUT FOR FIRST LAYER:\n",
    "layer1_outputs=np.dot(inputs,weights_array.T)+bais_array\n",
    "\n",
    "## CALUCLATE THE OUTPUT FOR SECOND LAYER\n",
    "layer2_outputs=np.dot(layer1_outputs,weight2_array.T)+baises2_array\n",
    "\n",
    "print(layer2_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>DENSE LAYER CLASS</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](IMG/class.jpg)\n",
    "![alt text](<IMG/Class dense laywe.jpg>)\n",
    "![alt text](<IMG/Initializing weights.jpg>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      " [-1.0475188e-04  1.1395361e-04 -4.7983500e-05]\n",
      " [-2.7414842e-04  3.1729150e-04 -8.6921798e-05]\n",
      " [-4.2188365e-04  5.2666257e-04 -5.5912682e-05]\n",
      " [-5.7707680e-04  7.1401405e-04 -8.9430439e-05]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "nnfs.init()\n",
    "\n",
    "#DENSE LAYER:\n",
    "class Layer_Dense:\n",
    "    #LAYER INITIALIZATION\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        #INITIALIZE WEIGHT AND BIASES\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    #FORWARD PASS\n",
    "    def forward(self, inputs):\n",
    "        #CALUCLATE OUTPUT VALUES FROM INPUTS,WEIGHTS AND BIASES\n",
    "        self.output=np.dot(inputs,self.weights)+self.biases\n",
    "#CREATE DATASET\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "#CREATE DENSE LAYER WITH 2 INPUT FEATURES AND # OUTPUT VALUES\n",
    "dense_layer1 = Layer_Dense(2, 3)\n",
    "# PERFORM a FORWARD PASS of OUR TRAINING DATA THROUGH THIS LAYER\n",
    "dense_layer1.forward(X)\n",
    "# LET'S SEE THE OUTPUT OF THE FIRST SAMPLES:\n",
    "print(dense_layer1.output[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Activation Function :Relu</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](IMG/Activation_functions.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.  2.  0.  3.3 0.  1.1 2.2 0. ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "inputs=[0,2,-1,3.3,-2.7,1.1,2.2,-100]\n",
    "output=np.maximum(0,inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relu Activation\n",
    "class Activation_Relu:\n",
    "    ## Forward Pass\n",
    "    def forward(self, inputs):\n",
    "        self.output=np.maximum(0,inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Activation Function :Softmax</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](IMG/softmax.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1\n",
    "![Step 1](<IMG/Screenshot 2024-10-27 101702.jpg>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# SETP 1\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m exp_values\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mexp(inputs\u001b[38;5;241m-\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[0;32m      3\u001b[0m exp_values\n",
      "File \u001b[1;32ma:\\LANGCHAIN\\langchainenv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2810\u001b[0m, in \u001b[0;36mmax\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2692\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_max_dispatcher)\n\u001b[0;32m   2693\u001b[0m \u001b[38;5;129m@set_module\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   2694\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmax\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[0;32m   2695\u001b[0m          where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[0;32m   2696\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2697\u001b[0m \u001b[38;5;124;03m    Return the maximum of an array or maximum along an axis.\u001b[39;00m\n\u001b[0;32m   2698\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2808\u001b[0m \u001b[38;5;124;03m    5\u001b[39;00m\n\u001b[0;32m   2809\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaximum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2811\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32ma:\\LANGCHAIN\\langchainenv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:88\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     86\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[1;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ufunc\u001b[38;5;241m.\u001b[39mreduce(obj, axis, dtype, out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n",
      "\u001b[1;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "# SETP 1\n",
    "exp_values=np.exp(inputs-np.max(inputs,axis=1,keepdims=True))\n",
    "exp_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2\n",
    "![alt text](<IMG/Screenshot 2024-10-27 100913.jpg>)\n",
    "![alt text](IMG/softmax_probalties.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06414769, 0.17437149, 0.47399085, 0.28748998],\n",
       "       [0.04517666, 0.90739747, 0.00224921, 0.04517666],\n",
       "       [0.00522984, 0.34875873, 0.63547983, 0.0105316 ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## STEP 2\n",
    "probabilities=exp_values/np.sum(exp_values,axis=1,keepdims=True)\n",
    "probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.06414769 0.17437149 0.47399085 0.28748998]\n",
      " [0.04517666 0.90739747 0.00224921 0.04517666]\n",
      " [0.00522984 0.34875873 0.63547983 0.0105316 ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Activation Softmax\n",
    "## Activation softmax is a common activation function used in the output layer of a neural network. It is used\n",
    "## to output a probability distribution over the classes. The softmax function is defined as follows:\n",
    "inputs=[[1,2,3,2.5],\n",
    "        [2.,5.,-1.,2],\n",
    "        [-1.5,2.7,3.3,-0.8]]\n",
    "## get unnormalized probabiltites\n",
    "exp_values=np.exp(inputs-np.max(inputs,axis=1,keepdims=True))\n",
    "## normalize them for each sample\n",
    "probabilities=exp_values/np.sum(exp_values,axis=1,keepdims=True)\n",
    "print(probabilities)\n",
    "np.sum(probabilities,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax Activation Fucntion\n",
    "class Activation_Softmax:\n",
    "    # FORWARD PASS\n",
    "    def forward(self,inputs):\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values=np.exp(inputs-np.max(inputs,axis=1,keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities=exp_values/np.sum(exp_values,axis=1,keepdims=True)\n",
    "        self.output=probabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>FORWARD PASS</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](IMG/forward_pass.jpg)\n",
    "![alt text](IMG/simplicztion_forward_pass.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333334]]\n"
     ]
    }
   ],
   "source": [
    "# CREATE DATASET\n",
    "X,y=spiral_data(samples=100,classes=3)\n",
    "# CREATE DENSE LAYER WITH INPUT FEATURES AND  OUTPUT VALUES\n",
    "\n",
    "dense1=Layer_Dense(2,3)\n",
    "# CREATE RELU ACTIVATION FUNCTION(TO BE USED WITH DENSE LAYER)\n",
    "\n",
    "activation1=Activation_Relu()\n",
    "\n",
    "# CREATE DENSE LAYER WITH OUTPUT FROM PREVIOUS LAYER AND 1 OUTPUT\n",
    "dense2=Layer_Dense(3,3)\n",
    "\n",
    "# CREATE SOFTMAX ACTIVATION FUNCTION\n",
    "activation2=Activation_Softmax()\n",
    "# MAKE A FORWARD PASS OF OUR TRAINING DATA THROUGH THIS LAYER\n",
    "\n",
    "dense1.forward(X)\n",
    "# MAKE A FORWARD PASS THROUGH ACTIVATION FUNCTION IT TAKES OUTPUT OF FIRST DENSE LAYER HERE\n",
    "\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# MAKE A FORWARD PASS THROUGH SECOND DENSE LAYER\n",
    "# IT TAKES FORWARD OUTPUTS OF ACTIVATION FUNCTION OF FIRST LAYER AS INPUTS\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# MAKE A FORWARD PASS THROUGH ACTIVATION FUNCTION IT TAKES OUTPUT OF SECOND DENSE LAYER HERE\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "# SEE THE OUTPUT OF THE FIRST FEW SAMPLES\n",
    "print(activation2.output[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>CALCULATING NETWORK ERROR WITH LOSS </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](IMG/croos_entropy_loss.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "CROSS ENTROPY LOSS BUILDING BLOCKS IN PYTHON\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7 0.5 0.9]\n"
     ]
    }
   ],
   "source": [
    "softmax_outputs=np.array([[0.7,0.1,0.2],\n",
    "                          [0.1,0.5,0.4],\n",
    "                          [0.02,0.9,0.08]])\n",
    "class_targets=[0,1,1]\n",
    "print(softmax_outputs[[0,1,2],class_targets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.35667494 0.69314718 0.10536052]\n",
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "print(-np.log(softmax_outputs[range(len(softmax_outputs)),class_targets]))\n",
    "neg_log=-np.log(softmax_outputs[range(len(softmax_outputs)),class_targets])\n",
    "average_loss=np.mean(neg_log)\n",
    "print(average_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "IF DATA IS ONE HOT ENCODED, HOW TO EXTRACT THE RELEVANT PREDICTIONS\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.35667494 0.69314718 0.10536052]\n",
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "y_true_check=np.array([\n",
    "    [1,0,0],\n",
    "    [0,1,0],\n",
    "    [0,1,0]\n",
    "])\n",
    "y_pred_clipped_check=np.array([\n",
    "    [0.7,0.2,0.1],\n",
    "    [0.8,0.5,0.4],\n",
    "    [0.02,0.9,0.08]\n",
    "])\n",
    "A=y_true_check*y_pred_clipped_check\n",
    "B=np.sum(A,axis=1)\n",
    "C=-np.log(B)\n",
    "print(C)\n",
    "print(np.mean(C))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "IMPLEMENTING THE CATEGORICAL CROSS ENTROPY CLASS\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "IMPLEMENTING THE LOSS CLASS\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common loss class\n",
    "class Loss:\n",
    " # Calculates the data and regularization losses\n",
    " # given model output and ground truth values\n",
    " def calculate(self, output, y):\n",
    "  # Calculate sample losses\n",
    "  sample_losses = self.forward(output, y)\n",
    "  # Calculate mean loss\n",
    "  data_loss = np.mean(sample_losses)\n",
    "  # Return loss\n",
    "  return data_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cross entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    # FORWAED PASS \n",
    "    def forward(self, y_pred, y_true):\n",
    "        # NUMBER OF SAMPLES IN A BATCH\n",
    "        samples=len(y_pred)\n",
    "        # CLIP DATA TO PREVENT DIVSION BY ZERO\n",
    "        # CLIP BOTH SIDES TO NOT DRAG  MEAN TOWARDS ANY VALUE\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "        # Probabilities for target values\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) ==1:\n",
    "            correct_confidences=y_pred_clipped[range(samples),y_true]\n",
    "        # MASK VALUE - ONLY FOR ONE HOT ENCODED LABELS\n",
    "        elif len(y_true.shape)==2:\n",
    "            correct_confidences=np.sum(y_pred_clipped*y_true, axis=1)\n",
    "        \n",
    "        neg_log_likelihoods=-np.log(correct_confidences)\n",
    "        return neg_log_likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "class_targets=np.array([\n",
    "    [1,0,0],\n",
    "    [0,1,0],\n",
    "    [0,1,0]\n",
    "])\n",
    "softmax_outputs=np.array([\n",
    "    [0.7,0.2,0.1],\n",
    "    [0.8,0.5,0.4],\n",
    "    [0.02,0.9,0.08]\n",
    "])\n",
    "loss_functions=Loss_CategoricalCrossentropy()\n",
    "loss=loss_functions.calculate(softmax_outputs,class_targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "FULL CODE UPTO THIS POINT\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333334]]\n",
      "Loss: 1.0986066\n",
      "Accuracy: 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "# CREATE DATA SET\n",
    "X,y=spiral_data(samples=100,classes=3)\n",
    "\n",
    "# CREATE DENSE LAYER 2 INPUTS FEATURES 3 OUTPUT VALUES\n",
    "DENSE1=Layer_Dense(2,3)\n",
    "\n",
    "# CRETAE A ACTIVATION RELU (TO BE USED WITH DENSE LAYER):\n",
    "ACTIVATION1=Activation_Relu()\n",
    "\n",
    "# CREATE A SECOND DENSE LAYER WITH 3 INPUTS FEATURES(AS WE TAKE OUTPUT OF PREVIOUS LAYER) AND OUTPUT 3 VALUES\n",
    "DENSE2=Layer_Dense(3,3)\n",
    "\n",
    "# CREATE SOFTMAX ACTIVATION(TO BE USED WITH DENSE LAYER):\n",
    "ACTIVATION2=Activation_Softmax()\n",
    "\n",
    "# CREATE LOSS FUNCTION\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "#PERFORM A FORWARD PASS OUR TRAINING DATA THROUGH THIS LAYER\n",
    "DENSE1.forward(X)\n",
    "\n",
    "# PERFORM A FORWARD PASS THROUGH ACTIVATION LAYER\n",
    "# IT TAKES THE OUTPUT OF FIRST DENSE LAYER HERE\n",
    "ACTIVATION1.forward(DENSE1.output)\n",
    "\n",
    "# PERFORM A FORWARD PASS THROUGH SECOND DENSE  LAYER\n",
    "# IT TAKES THE OUTPUT OF ACTIVATION LAYER HERE\n",
    "DENSE2.forward(activation1.output)\n",
    "\n",
    "# PERFORM A FORWARD PASS THROUGH ACTIVATION FUNCTION\n",
    "# IT TAKES THE OUTPUT OF SECOND DENSE LAYER HERE\n",
    "ACTIVATION2.forward(dense2.output)\n",
    "\n",
    "# LET\"S SEE OUTPUT OF THE  FIRST SAMPLES\n",
    "print(ACTIVATION2.output[:5])\n",
    "\n",
    "# TAKE THE OUTPUT OF SECOND DENSE LAYER HERE AND RETURN LOSS\n",
    "loss=loss_function.calculate(ACTIVATION2.output,y)\n",
    "\n",
    "# PRINT  LOSS VALUE\n",
    "print(\"Loss:\", loss)\n",
    "\n",
    "# CALCULATE ACCURACY FROM OUPUT OF ACTIVATIONS2 AND TARGERTS\n",
    "# CALCULATE VALUES ALONG FIRST AXIS\n",
    "predictions=np.argmax(activation2.output,axis=1)\n",
    "if len(y.shape)==2:\n",
    "    y=np.argmax(y,axis=1)\n",
    "accuracy=np.mean(predictions==y)\n",
    "# PRINT ACCURACY\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>INTRODUCING ACCRUACY </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1]\n",
      "acc: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Probabilities of 3 samples\n",
    "softmax_outputs = np.array([[0.7, 0.2, 0.1],\n",
    " [0.5, 0.1, 0.4],\n",
    " [0.02, 0.9, 0.08]])\n",
    "# Target (ground-truth) labels for 3 samples\n",
    "class_targets = np.array([0, 1, 1])\n",
    "# Calculate values along second axis (axis of index 1)\n",
    "predictions = np.argmax(softmax_outputs, axis=1)\n",
    "print(predictions)\n",
    "# If targets are one-hot encoded - convert them\n",
    "if len(class_targets.shape) == 2:\n",
    " class_targets = np.argmax(class_targets, axis=1)\n",
    "# True evaluates to 1; False to 0\n",
    "accuracy = np.mean(predictions == class_targets)\n",
    "print('acc:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>STRATEGY 1: RANDOMONLY SELECT WEIGHTS AND BAISES - DOES NOT WORK </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](IMG/strategy1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "New set of weight found,iteration : 0 loss : 1.0986066 acc : 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "# CREATE DATASET\n",
    "import nnfs\n",
    "from nnfs.datasets import vertical_data\n",
    "X,y=vertical_data(samples=100,classes=3)\n",
    "\n",
    "# CREATE DENSE LAYER \n",
    "DENSE1=Layer_Dense(2,3) #FIRST DENSE ,2 INPUTS\n",
    "\n",
    "# APPLYING RELU FUNCTION\n",
    "ACTIVATION1=Activation_Relu()\n",
    "\n",
    "# SECON DENSE LAYER 3 INPUTS ,# OUTPUTS\n",
    "DENSE2=Layer_Dense(3,3)\n",
    "\n",
    "# ACTIVATION SOFTMAX\n",
    "ACTIVATION2=Activation_Softmax()\n",
    "\n",
    "# CREATE A LOSS FUNCTION\n",
    "loss_function=Loss_CategoricalCrossentropy()\n",
    "\n",
    "# HEPER VAIRABLES\n",
    "lowest_loss=9999999\n",
    "# SOME INITAL VALUE\n",
    "best_dense1_weights=dense1.weights.copy()\n",
    "best_dense1_biases=dense1.biases.copy()\n",
    "best_dense2_weights=dense2.weights.copy()\n",
    "best_dense2_biases=dense2.biases.copy()\n",
    "\n",
    "for iteration in range(100000):\n",
    "    # GENERATE A NEW SET OF WEIGHTS FOR ITERATIONS\n",
    "    dense1.weights=0.05*np.random.randn(2,3)\n",
    "    dense1.biases=0.05*np.random.randn(1,3)\n",
    "    dense2.weights=0.05*np.random.randn(3,3)\n",
    "    dense2.biases=0.05*np.random.randn(1,3)\n",
    "\n",
    "    # PERFORM a FROWARD PASS OF THE TRAINING DATA THROUGH THIS LAYER\n",
    "    DENSE1.forward(X)\n",
    "    ACTIVATION1.forward(DENSE1.output)\n",
    "    DENSE2.forward(ACTIVATION1.output)\n",
    "    ACTIVATION2.forward(DENSE2.output)\n",
    "\n",
    "    # PERFORM A FORWARD PASS THROUGH ACTIVSTION FUNCTION\n",
    "    # IT TAKES THE OUTPUT OF SECOND DENSE LAYER HERE AND RETURN THE LOSS\n",
    "    loss=loss_function.calculate(activation2.output,y)\n",
    "    \n",
    "    # CALCULATE THE ACCURACY FROM OUTPUT OF ACTIVATION2 AND TARGETS\n",
    "    # CALCULATE VALUES ALONG FIRST AXIS\n",
    "    predictions=np.argmax(activation2.output,axis=1)\n",
    "    accuracy=np.mean(predictions==y)\n",
    "\n",
    "    # IF LOSS IS SMALLER  - PRINT AND SAVE WEIGHTS AND BAISES ASIDE\n",
    "    if loss < lowest_loss:\n",
    "        print(iteration)\n",
    "        print(\"New set of weight found,iteration :\",iteration,\"loss :\",loss,\"acc :\",accuracy)\n",
    "        best_dense1_weights=dense1.weights.copy()\n",
    "        best_dense1_biases=dense1.biases.copy()\n",
    "        best_dense2_weights=dense2.weights.copy()\n",
    "        best_dense2_biases=dense2.biases.copy()\n",
    "        lowest_loss=loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>STRATEGY 2: FOR SPIRAL DATASET - DOES WORK </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](IMG/stragey2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "New set of weight found,iteration : 0 loss : 1.0986066 acc : 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "# CREATE DATASET\n",
    "import nnfs\n",
    "from nnfs.datasets import vertical_data\n",
    "X,y=vertical_data(samples=100,classes=3)\n",
    "\n",
    "# CREATE DENSE LAYER \n",
    "DENSE1=Layer_Dense(2,3) #FIRST DENSE ,2 INPUTS\n",
    "\n",
    "# APPLYING RELU FUNCTION\n",
    "ACTIVATION1=Activation_Relu()\n",
    "\n",
    "# SECON DENSE LAYER 3 INPUTS ,# OUTPUTS\n",
    "DENSE2=Layer_Dense(3,3)\n",
    "\n",
    "# ACTIVATION SOFTMAX\n",
    "ACTIVATION2=Activation_Softmax()\n",
    "\n",
    "# CREATE A LOSS FUNCTION\n",
    "loss_function=Loss_CategoricalCrossentropy()\n",
    "\n",
    "# HEPER VAIRABLES\n",
    "lowest_loss=9999999\n",
    "# SOME INITAL VALUE\n",
    "best_dense1_weights=dense1.weights.copy()\n",
    "best_dense1_biases=dense1.biases.copy()\n",
    "best_dense2_weights=dense2.weights.copy()\n",
    "best_dense2_biases=dense2.biases.copy()\n",
    "\n",
    "for iteration in range(100000):\n",
    "    # GENERATE A NEW SET OF WEIGHTS FOR ITERATIONS\n",
    "    dense1.weights+=0.05*np.random.randn(2,3)\n",
    "    dense1.biases+=0.05*np.random.randn(1,3)\n",
    "    dense2.weights+=0.05*np.random.randn(3,3)\n",
    "    dense2.biases+=0.05*np.random.randn(1,3)\n",
    "\n",
    "    # PERFORM a FROWARD PASS OF THE TRAINING DATA THROUGH THIS LAYER\n",
    "    DENSE1.forward(X)\n",
    "    ACTIVATION1.forward(DENSE1.output)\n",
    "    DENSE2.forward(ACTIVATION1.output)\n",
    "    ACTIVATION2.forward(DENSE2.output)\n",
    "\n",
    "    # PERFORM A FORWARD PASS THROUGH ACTIVSTION FUNCTION\n",
    "    # IT TAKES THE OUTPUT OF SECOND DENSE LAYER HERE AND RETURN THE LOSS\n",
    "    loss=loss_function.calculate(activation2.output,y)\n",
    "    \n",
    "    # CALCULATE THE ACCURACY FROM OUTPUT OF ACTIVATION2 AND TARGETS\n",
    "    # CALCULATE VALUES ALONG FIRST AXIS\n",
    "    predictions=np.argmax(activation2.output,axis=1)\n",
    "    accuracy=np.mean(predictions==y)\n",
    "\n",
    "    # IF LOSS IS SMALLER  - PRINT AND SAVE WEIGHTS AND BAISES ASIDE\n",
    "    if loss < lowest_loss:\n",
    "        print(iteration)\n",
    "        print(\"New set of weight found,iteration :\",iteration,\"loss :\",loss,\"acc :\",accuracy)\n",
    "        best_dense1_weights=dense1.weights.copy()\n",
    "        best_dense1_biases=dense1.biases.copy()\n",
    "        best_dense2_weights=dense2.weights.copy()\n",
    "        best_dense2_biases=dense2.biases.copy()\n",
    "        lowest_loss=loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Back Progation For Single Nueruon</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](IMG/Backprogation.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1,Loss: 36.0\n",
      "Iteration 2,Loss: 33.872397424621624\n",
      "Iteration 3,Loss: 31.87054345809546\n",
      "Iteration 4,Loss: 29.98699091998773\n",
      "Iteration 5,Loss: 28.214761511794592\n",
      "Iteration 6,Loss: 26.54726775906168\n",
      "Iteration 7,Loss: 24.978326552541866\n",
      "Iteration 8,Loss: 23.5021050739742\n",
      "Iteration 9,Loss: 22.11313179151597\n",
      "Iteration 10,Loss: 20.806246424284897\n",
      "Iteration 11,Loss: 19.576596334671486\n",
      "Iteration 12,Loss: 18.41961908608719\n",
      "Iteration 13,Loss: 17.33101994032309\n",
      "Iteration 14,Loss: 16.306757070164853\n",
      "Iteration 15,Loss: 15.343027506224132\n",
      "Iteration 16,Loss: 14.436253786815284\n",
      "Iteration 17,Loss: 13.583071280700132\n",
      "Iteration 18,Loss: 12.780312744165439\n",
      "Iteration 19,Loss: 12.024995767388878\n",
      "Iteration 20,Loss: 11.314319082257104\n",
      "final weights [-3.18248226 -0.63503547  1.45255321]\n",
      "Final bais 0.8175177371478247\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "## INTIAL PARAMETERS\n",
    "weights=np.array([-3.0,-1.0,2.0])\n",
    "bias=1.0\n",
    "inputs=np.array([1.0,-2.0,3.0])\n",
    "target_output=0.0\n",
    "learning_rate=0.001\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "def relu_derivative(x):\n",
    "    return np.where(x>0,1.0,0.0)\n",
    "for iteration in range(20):\n",
    "    \n",
    "    #FORWARD PASS\n",
    "    linear_output=np.dot(weights,inputs)+bias\n",
    "    output=relu(linear_output)\n",
    "    loss=(output-target_output) ** 2\n",
    "\n",
    "    #BACKWARD PASS\n",
    "    dloss_doutput=2*(output-target_output)\n",
    "    doutput_dlinear=relu_derivative(dloss_doutput)\n",
    "    dlinear_dweights=inputs\n",
    "    dlinear_dbias=1.0\n",
    "\n",
    "    dloss_dlinear=dloss_doutput * doutput_dlinear\n",
    "    dloss_dweight=dloss_dlinear * dlinear_dweights\n",
    "    dloss_bias=dloss_dlinear * dlinear_dbias\n",
    "\n",
    "    #UPDATE WEIGHTS AND BIAS\n",
    "    weights-=learning_rate*dloss_dweight\n",
    "    bias-=learning_rate*dloss_bias\n",
    "\n",
    "    #PRINT THE LOSS FOR THIS ITERATION\n",
    "    print(f\"Iteration {iteration+1},Loss: {loss}\")\n",
    "print(\"final weights\",weights)\n",
    "print(\"Final bais\",bias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>ADDING THE BACKWARD METHOD IN THE RELU ACTIVATION CLASS</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](<IMG/backprogration on the relu.jpg>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relu Activation\n",
    "class Activation_Relu:\n",
    "    ## Forward Pass\n",
    "    def forward(self, inputs):\n",
    "        self.output=np.maximum(0,inputs)\n",
    "    # BACKWARD PASS\n",
    "    def backward(self,dvalues):\n",
    "        #SINCE WE NEED TO MODITFY THE ORIGINAL VARIABLE\n",
    "        # LET MAKE A COPY OF THE VALUES\n",
    "        self.dinputs=dvalues.copy()\n",
    "        # ZERO GRADIENT WHERE INPUT VALUES WERE NEGATIVE\n",
    "        self.dinputs[self.inputs<=0]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CROSS ENTROPY LOSS\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    #BACKWARD PASS\n",
    "    def backward(self,dvalues,y_true):\n",
    "        #NUMBER OF SAMPLES\n",
    "        samples = len(dvalues)\n",
    "        # NUMBER OF LABELS IN EVERY SAMPLE\n",
    "        # WE WILL USE THE FIRST SAMPLE TO COUNT THEM\n",
    "        labels=len(dvalues[0])\n",
    "        # IF LABELS ARE SPARSE,TURN THEM INTO ONE-HOT VECTOR\n",
    "        if len(y_true.shape)==1:\n",
    "            y_true=np.eye(labels)[y_true]\n",
    "            # CALCULATE GRADIENT\n",
    "            self.dinputs=-y_true/dvalues\n",
    "            # NORMALIZE\n",
    "            self.dinputs=self.dinputs/samples\n",
    "\n",
    "         "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
