{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " `BUILDING NEURAL NETWORKS FROM SCRATCH PART 1: CODING NEURONS AND LAYERS`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>CODING OUR FIRST NEURON: 3 INPUTS</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](IMG/nueron_with_3_inputs.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3\n"
     ]
    }
   ],
   "source": [
    "inputs=[1,2,3]\n",
    "wieghts=[0.2,0.8,-0.5]\n",
    "bais=2\n",
    "\n",
    "output=(inputs[0]*wieghts[0]+inputs[1]*wieghts[1]+inputs[2]*wieghts[2]+bais)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "SINGLE NEURON USING NUMPY\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](IMG/single_neuron_with_numpy.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "Inputs=[1.0,2.0,3.0,2.5]\n",
    "Weights=[0.2,0.8,-0.5,1.0]\n",
    "bais=2.0\n",
    "output=np.dot(Inputs,Weights)+bais\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>CODING OUR SECOND NEURON: 4 INPUTS</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](IMG/nueron_with_4_inputs.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.8\n"
     ]
    }
   ],
   "source": [
    "inputs=[1.0,2.0,3.0,2.5]\n",
    "weights=[0.2,0.8,-0.5,1.0]\n",
    "bais=2.0\n",
    "output=(inputs[0]*weights[0]+\n",
    "        inputs[1]*weights[1]+\n",
    "        inputs[2]*weights[2]+\n",
    "        inputs[3]*weights[3]+\n",
    "        bais)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>CODING OUR FIRST LAYER</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](IMG/Layer_of_neuron.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8, 1.21, 2.385]\n"
     ]
    }
   ],
   "source": [
    "inputs = [1, 2, 3, 2.5]\n",
    "\n",
    "weights = [[0.2, 0.8, -0.5, 1],\n",
    " [0.5, -0.91, 0.26, -0.5],\n",
    " [-0.26, -0.27, 0.17, 0.87]]\n",
    "\n",
    "weights1 = weights[0] #LIST OF WEIGHTS ASSOCIATED WITH 1ST NEURON : W11, W12, W13, W14\n",
    "weights2 = weights[1] #LIST OF WEIGHTS ASSOCIATED WITH 2ND NEURON : W21, W22, W23, W24\n",
    "weights3 = weights[2] #LIST OF WEIGHTS ASSOCIATED WITH 3RD NEURON : W31, W32, W33, W34\n",
    "\n",
    "biases = [2, 3, 0.5]\n",
    "\n",
    "bias1 = 2\n",
    "bias2 = 3\n",
    "bias3 = 0.5\n",
    "\n",
    "outputs = [\n",
    " # Neuron 1:\n",
    " inputs[0]*weights1[0] +\n",
    " inputs[1]*weights1[1] +\n",
    " inputs[2]*weights1[2] +\n",
    " inputs[3]*weights1[3] + bias1,\n",
    " # Neuron 2:\n",
    " inputs[0]*weights2[0] +\n",
    " inputs[1]*weights2[1] +\n",
    " inputs[2]*weights2[2] +\n",
    " inputs[3]*weights2[3] + bias2,\n",
    " # Neuron 3:\n",
    " inputs[0]*weights3[0] +\n",
    " inputs[1]*weights3[1] +\n",
    " inputs[2]*weights3[2] +\n",
    " inputs[3]*weights3[3] + bias3]\n",
    "\n",
    "print(outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "USING LOOPS FOR BETTER AND EASIER CODING</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8, 6.01, 8.395]\n"
     ]
    }
   ],
   "source": [
    "inputs = [1, 2, 3, 2.5]\n",
    "\n",
    "##LIST OF WEIGHTS\n",
    "weights = [[0.2, 0.8, -0.5, 1],\n",
    " [0.5, -0.91, 0.26, -0.5],\n",
    " [-0.26, -0.27, 0.17, 0.87]]\n",
    "\n",
    "##LIST OF BIASES\n",
    "biases = [2, 3, 0.5]\n",
    "\n",
    "# Output of current layer\n",
    "neuron_output = 0\n",
    "layer_outputs=[]\n",
    "# For each neuron\n",
    "for neuron_weights, neuron_bias in zip(weights, biases):\n",
    "    for weight,n_inputs in zip(neuron_weights,inputs):\n",
    "         # Multiply this input by associated weight\n",
    "         # and add to the neuron's output variable\n",
    "        neuron_output += n_inputs*weight ## W31*X1 + W32*X2 + W33*X3 + W34*X4\n",
    "   # Add bias\n",
    "    neuron_output += neuron_bias ## ## W31*X1 + W32*X2 + W33*X3 + W34*X4 + B3\n",
    "\n",
    "    layer_outputs.append(neuron_output)    \n",
    "print(layer_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "LAYER OF NEURONS USING NUMPY\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](<IMG/Screenshot 2024-10-25 152610.jpg>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8   1.21  2.385]\n"
     ]
    }
   ],
   "source": [
    "inputs = [1.0, 2.0, 3.0, 2.5]\n",
    "weights = [[0.2, 0.8, -0.5, 1],\n",
    " [0.5, -0.91, 0.26, -0.5],\n",
    " [-0.26, -0.27, 0.17, 0.87]]\n",
    "biases = [2.0, 3.0, 0.5]\n",
    "\n",
    "# A dot product of a matrix and a vector results in a list of dot products. \n",
    "#The np.dot() method treats the matrix as a list of vectors and performs a dot product of each of those vectors with the other vector\n",
    "layer_outputs = np.dot(weights, inputs) + biases\n",
    "print(layer_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "LAYER OF NEURONS AND BATCH OF DATA USING NUMPY\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](IMG/Batch_of_inputs.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "NEED TO TAKE TRANSPOSE OF WEIGHT MATRIX</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](IMG/transpose_of.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.8    1.21   2.385]\n",
      " [ 8.9   -1.81   0.2  ]\n",
      " [ 1.41   1.051  0.026]]\n"
     ]
    }
   ],
   "source": [
    "## Batch of Inputs\n",
    "inputs=[[1.0,2.0,3.0,2.5],\n",
    "        [2.0,5.0,-1.0,2.0],\n",
    "        [-1.5,2.7,3.3,-0.8]]\n",
    "weights=[[0.2,0.8,-0.5,1],\n",
    "         [0.5,-0.91,0.26,-0.5],\n",
    "         [-0.26,-0.27,0.17,0.87]]\n",
    "bais=[2.0,3.0,0.5]\n",
    "# NOTE: We cant Transpose Lists in python so we have the convert the weights matrix into array first\n",
    "outputs=np.dot(inputs,np.array(weights).T)+bais\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "2 LAYERS AND BATCH OF DATA USING NUMPY\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](IMG/Two_layers.jpg)\n",
    "![alt text](IMG/weight_of_two_layer.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5031  -1.04185 -2.03875]\n",
      " [ 0.2434  -2.7332  -5.7633 ]\n",
      " [-0.99314  1.41254 -0.35655]]\n"
     ]
    }
   ],
   "source": [
    "## Batch of Inputs\n",
    "inputs=[[1.0,2.0,3.0,2.5],\n",
    "        [2.0,5.0,-1.0,2.0],\n",
    "        [-1.5,2.7,3.3,-0.8]]\n",
    "## First Layers Weights\n",
    "weights=[[0.2,0.8,-0.5,1],\n",
    "         [0.5,-0.91,0.26,-0.5],\n",
    "         [-0.26,-0.27,0.17,0.87]]\n",
    "\n",
    "baise1=[2,3,0.5]\n",
    "\n",
    "## Second Layers Weights\n",
    "weights2=[[0.1,-0.14,0.5],\n",
    "          [-0.5,0.12,-0.33],\n",
    "          [-0.44,0.73,-0.13]\n",
    "        ]\n",
    "bais2=[-1,2,-0.5]\n",
    "\n",
    "## Convert lists to numpy array\n",
    "inputs_array=np.array(inputs)\n",
    "weights_array=np.array(weights)\n",
    "bais_array=np.array(baise1)\n",
    "weight2_array=np.array(weights2)\n",
    "baises2_array=np.array(bais2)\n",
    "\n",
    "## CALCULATE THE OUTPUT FOR FIRST LAYER:\n",
    "layer1_outputs=np.dot(inputs,weights_array.T)+bais_array\n",
    "\n",
    "## CALUCLATE THE OUTPUT FOR SECOND LAYER\n",
    "layer2_outputs=np.dot(layer1_outputs,weight2_array.T)+baises2_array\n",
    "\n",
    "print(layer2_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>DENSE LAYER CLASS</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](IMG/class.jpg)\n",
    "![alt text](<IMG/Class dense laywe.jpg>)\n",
    "![alt text](<IMG/Initializing weights.jpg>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      " [-1.0475188e-04  1.1395361e-04 -4.7983500e-05]\n",
      " [-2.7414842e-04  3.1729150e-04 -8.6921798e-05]\n",
      " [-4.2188365e-04  5.2666257e-04 -5.5912682e-05]\n",
      " [-5.7707680e-04  7.1401405e-04 -8.9430439e-05]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "nnfs.init()\n",
    "\n",
    "#DENSE LAYER:\n",
    "class Layer_Dense:\n",
    "    #LAYER INITIALIZATION\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        #INITIALIZE WEIGHT AND BIASES\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    #FORWARD PASS\n",
    "    def forward(self, inputs):\n",
    "        #CALUCLATE OUTPUT VALUES FROM INPUTS,WEIGHTS AND BIASES\n",
    "        self.output=np.dot(inputs,self.weights)+self.biases\n",
    "#CREATE DATASET\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "#CREATE DENSE LAYER WITH 2 INPUT FEATURES AND # OUTPUT VALUES\n",
    "dense_layer1 = Layer_Dense(2, 3)\n",
    "# PERFORM a FORWARD PASS of OUR TRAINING DATA THROUGH THIS LAYER\n",
    "dense_layer1.forward(X)\n",
    "# LET'S SEE THE OUTPUT OF THE FIRST SAMPLES:\n",
    "print(dense_layer1.output[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Activation Function :Relu</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](IMG/Activation_functions.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.  2.  0.  3.3 0.  1.1 2.2 0. ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "inputs=[0,2,-1,3.3,-2.7,1.1,2.2,-100]\n",
    "output=np.maximum(0,inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relu Activation\n",
    "class Activation_Relu:\n",
    "    ## Forward Pass\n",
    "    def forward(self, inputs):\n",
    "        self.output=np.maximum(0,inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Activation Function :Softmax</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](IMG/softmax.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1\n",
    "![Step 1](<IMG/Screenshot 2024-10-27 101702.jpg>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.13533528, 0.36787944, 1.        , 0.60653066],\n",
       "       [0.04978707, 1.        , 0.00247875, 0.04978707],\n",
       "       [0.00822975, 0.54881164, 1.        , 0.01657268]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SETP 1\n",
    "exp_values=np.exp(inputs-np.max(inputs,axis=1,keepdims=True))\n",
    "exp_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2\n",
    "![alt text](<IMG/Screenshot 2024-10-27 100913.jpg>)\n",
    "![alt text](IMG/softmax_probalties.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06414769, 0.17437149, 0.47399085, 0.28748998],\n",
       "       [0.04517666, 0.90739747, 0.00224921, 0.04517666],\n",
       "       [0.00522984, 0.34875873, 0.63547983, 0.0105316 ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## STEP 2\n",
    "probabilities=exp_values/np.sum(exp_values,axis=1,keepdims=True)\n",
    "probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.06414769 0.17437149 0.47399085 0.28748998]\n",
      " [0.04517666 0.90739747 0.00224921 0.04517666]\n",
      " [0.00522984 0.34875873 0.63547983 0.0105316 ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Activation Softmax\n",
    "## Activation softmax is a common activation function used in the output layer of a neural network. It is used\n",
    "## to output a probability distribution over the classes. The softmax function is defined as follows:\n",
    "inputs=[[1,2,3,2.5],\n",
    "        [2.,5.,-1.,2],\n",
    "        [-1.5,2.7,3.3,-0.8]]\n",
    "## get unnormalized probabiltites\n",
    "exp_values=np.exp(inputs-np.max(inputs,axis=1,keepdims=True))\n",
    "## normalize them for each sample\n",
    "probabilities=exp_values/np.sum(exp_values,axis=1,keepdims=True)\n",
    "print(probabilities)\n",
    "np.sum(probabilities,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax Activation Fucntion\n",
    "class Activation_Softmax:\n",
    "    # FORWARD PASS\n",
    "    def forward(self,inputs):\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values=np.exp(inputs-np.max(inputs,axis=1,keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities=exp_values/np.sum(exp_values,axis=1,keepdims=True)\n",
    "        self.output=probabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>FORWARD PASS</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](IMG/forward_pass.jpg)\n",
    "![alt text](IMG/simplicztion_forward_pass.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333334]]\n"
     ]
    }
   ],
   "source": [
    "# CREATE DATASET\n",
    "X,y=spiral_data(samples=100,classes=3)\n",
    "# CREATE DENSE LAYER WITH INPUT FEATURES AND  OUTPUT VALUES\n",
    "\n",
    "dense1=Layer_Dense(2,3)\n",
    "# CREATE RELU ACTIVATION FUNCTION(TO BE USED WITH DENSE LAYER)\n",
    "\n",
    "activation1=Activation_Relu()\n",
    "\n",
    "# CREATE DENSE LAYER WITH OUTPUT FROM PREVIOUS LAYER AND 1 OUTPUT\n",
    "dense2=Layer_Dense(3,3)\n",
    "\n",
    "# CREATE SOFTMAX ACTIVATION FUNCTION\n",
    "activation2=Activation_Softmax()\n",
    "# MAKE A FORWARD PASS OF OUR TRAINING DATA THROUGH THIS LAYER\n",
    "\n",
    "dense1.forward(X)\n",
    "# MAKE A FORWARD PASS THROUGH ACTIVATION FUNCTION IT TAKES OUTPUT OF FIRST DENSE LAYER HERE\n",
    "\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# MAKE A FORWARD PASS THROUGH SECOND DENSE LAYER\n",
    "# IT TAKES FORWARD OUTPUTS OF ACTIVATION FUNCTION OF FIRST LAYER AS INPUTS\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# MAKE A FORWARD PASS THROUGH ACTIVATION FUNCTION IT TAKES OUTPUT OF SECOND DENSE LAYER HERE\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "# SEE THE OUTPUT OF THE FIRST FEW SAMPLES\n",
    "print(activation2.output[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>CALCULATING NETWORK ERROR WITH LOSS </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](IMG/croos_entropy_loss.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "CROSS ENTROPY LOSS BUILDING BLOCKS IN PYTHON\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7 0.5 0.9]\n"
     ]
    }
   ],
   "source": [
    "softmax_outputs=np.array([[0.7,0.1,0.2],\n",
    "                          [0.1,0.5,0.4],\n",
    "                          [0.02,0.9,0.08]])\n",
    "class_targets=[0,1,1]\n",
    "print(softmax_outputs[[0,1,2],class_targets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.35667494 0.69314718 0.10536052]\n",
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "print(-np.log(softmax_outputs[range(len(softmax_outputs)),class_targets]))\n",
    "neg_log=-np.log(softmax_outputs[range(len(softmax_outputs)),class_targets])\n",
    "average_loss=np.mean(neg_log)\n",
    "print(average_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "IF DATA IS ONE HOT ENCODED, HOW TO EXTRACT THE RELEVANT PREDICTIONS\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.35667494 0.69314718 0.10536052]\n",
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "y_true_check=np.array([\n",
    "    [1,0,0],\n",
    "    [0,1,0],\n",
    "    [0,1,0]\n",
    "])\n",
    "y_pred_clipped_check=np.array([\n",
    "    [0.7,0.2,0.1],\n",
    "    [0.8,0.5,0.4],\n",
    "    [0.02,0.9,0.08]\n",
    "])\n",
    "A=y_true_check*y_pred_clipped_check\n",
    "B=np.sum(A,axis=1)\n",
    "C=-np.log(B)\n",
    "print(C)\n",
    "print(np.mean(C))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "IMPLEMENTING THE CATEGORICAL CROSS ENTROPY CLASS\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "IMPLEMENTING THE LOSS CLASS\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common loss class\n",
    "class Loss:\n",
    " # Calculates the data and regularization losses\n",
    " # given model output and ground truth values\n",
    " def calculate(self, output, y):\n",
    "  # Calculate sample losses\n",
    "  sample_losses = self.forward(output, y)\n",
    "  # Calculate mean loss\n",
    "  data_loss = np.mean(sample_losses)\n",
    "  # Return loss\n",
    "  return data_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cross entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    # FORWAED PASS \n",
    "    def forward(self, y_pred, y_true):\n",
    "        # NUMBER OF SAMPLES IN A BATCH\n",
    "        samples=len(y_pred)\n",
    "        # CLIP DATA TO PREVENT DIVSION BY ZERO\n",
    "        # CLIP BOTH SIDES TO NOT DRAG  MEAN TOWARDS ANY VALUE\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "        # Probabilities for target values\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) ==1:\n",
    "            correct_confidences=y_pred_clipped[range(samples),y_true]\n",
    "        # MASK VALUE - ONLY FOR ONE HOT ENCODED LABELS\n",
    "        elif len(y_true.shape)==2:\n",
    "            correct_confidences=np.sum(y_pred_clipped*y_true, axis=1)\n",
    "        \n",
    "        neg_log_likelihoods=-np.log(correct_confidences)\n",
    "        return neg_log_likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "class_targets=np.array([\n",
    "    [1,0,0],\n",
    "    [0,1,0],\n",
    "    [0,1,0]\n",
    "])\n",
    "softmax_outputs=np.array([\n",
    "    [0.7,0.2,0.1],\n",
    "    [0.8,0.5,0.4],\n",
    "    [0.02,0.9,0.08]\n",
    "])\n",
    "loss_functions=Loss_CategoricalCrossentropy()\n",
    "loss=loss_functions.calculate(softmax_outputs,class_targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "FULL CODE UPTO THIS POINT\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333334]]\n",
      "Loss: 1.0986066\n",
      "Accuracy: 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "# CREATE DATA SET\n",
    "X,y=spiral_data(samples=100,classes=3)\n",
    "\n",
    "# CREATE DENSE LAYER 2 INPUTS FEATURES 3 OUTPUT VALUES\n",
    "DENSE1=Layer_Dense(2,3)\n",
    "\n",
    "# CRETAE A ACTIVATION RELU (TO BE USED WITH DENSE LAYER):\n",
    "ACTIVATION1=Activation_Relu()\n",
    "\n",
    "# CREATE A SECOND DENSE LAYER WITH 3 INPUTS FEATURES(AS WE TAKE OUTPUT OF PREVIOUS LAYER) AND OUTPUT 3 VALUES\n",
    "DENSE2=Layer_Dense(3,3)\n",
    "\n",
    "# CREATE SOFTMAX ACTIVATION(TO BE USED WITH DENSE LAYER):\n",
    "ACTIVATION2=Activation_Softmax()\n",
    "\n",
    "# CREATE LOSS FUNCTION\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "#PERFORM A FORWARD PASS OUR TRAINING DATA THROUGH THIS LAYER\n",
    "DENSE1.forward(X)\n",
    "\n",
    "# PERFORM A FORWARD PASS THROUGH ACTIVATION LAYER\n",
    "# IT TAKES THE OUTPUT OF FIRST DENSE LAYER HERE\n",
    "ACTIVATION1.forward(DENSE1.output)\n",
    "\n",
    "# PERFORM A FORWARD PASS THROUGH SECOND DENSE  LAYER\n",
    "# IT TAKES THE OUTPUT OF ACTIVATION LAYER HERE\n",
    "DENSE2.forward(activation1.output)\n",
    "\n",
    "# PERFORM A FORWARD PASS THROUGH ACTIVATION FUNCTION\n",
    "# IT TAKES THE OUTPUT OF SECOND DENSE LAYER HERE\n",
    "ACTIVATION2.forward(dense2.output)\n",
    "\n",
    "# LET\"S SEE OUTPUT OF THE  FIRST SAMPLES\n",
    "print(ACTIVATION2.output[:5])\n",
    "\n",
    "# TAKE THE OUTPUT OF SECOND DENSE LAYER HERE AND RETURN LOSS\n",
    "loss=loss_function.calculate(ACTIVATION2.output,y)\n",
    "\n",
    "# PRINT  LOSS VALUE\n",
    "print(\"Loss:\", loss)\n",
    "\n",
    "# CALCULATE ACCURACY FROM OUPUT OF ACTIVATIONS2 AND TARGERTS\n",
    "# CALCULATE VALUES ALONG FIRST AXIS\n",
    "predictions=np.argmax(activation2.output,axis=1)\n",
    "if len(y.shape)==2:\n",
    "    y=np.argmax(y,axis=1)\n",
    "accuracy=np.mean(predictions==y)\n",
    "# PRINT ACCURACY\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>INTRODUCING ACCRUACY </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1]\n",
      "acc: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Probabilities of 3 samples\n",
    "softmax_outputs = np.array([[0.7, 0.2, 0.1],\n",
    " [0.5, 0.1, 0.4],\n",
    " [0.02, 0.9, 0.08]])\n",
    "# Target (ground-truth) labels for 3 samples\n",
    "class_targets = np.array([0, 1, 1])\n",
    "# Calculate values along second axis (axis of index 1)\n",
    "predictions = np.argmax(softmax_outputs, axis=1)\n",
    "print(predictions)\n",
    "# If targets are one-hot encoded - convert them\n",
    "if len(class_targets.shape) == 2:\n",
    " class_targets = np.argmax(class_targets, axis=1)\n",
    "# True evaluates to 1; False to 0\n",
    "accuracy = np.mean(predictions == class_targets)\n",
    "print('acc:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
