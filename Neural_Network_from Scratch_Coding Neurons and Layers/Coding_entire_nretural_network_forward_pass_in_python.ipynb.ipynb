{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>CREATING LAYERS: FORWARD AND BACKWARD PASS</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DENSE LAYER\n",
    "class Layer_Dense:\n",
    "    # LAYER INITIALIZATION\n",
    "    def __init__(self,n_inputs,n_neurons):\n",
    "        # INITIALIZE WEIGHTS AND BIASES\n",
    "        self.weights=0.01*np.random.randn(n_inputs,n_neurons)\n",
    "        self.biases=np.zeros((1,n_neurons))\n",
    "    # FORWARD PASS\n",
    "    def forward(self,inputs):\n",
    "        # REMEMBER INPUT VALUES\n",
    "        self.inputs=inputs\n",
    "        # CALULATE OUTPUT VALUES FROM INPUTS ONES,WEIGHTS AND BIASES\n",
    "        self.output=np.dot(inputs,self.weights)+self.biases\n",
    "    # BACKWARD PASS\n",
    "    def backward(self, dvalues):\n",
    "        # GRADIENT ON PARAMETERS\n",
    "        self.dweights=np.dot(self.inputs.T,dvalues)\n",
    "        self.dbiases=np.sum(dvalues,axis=0,keepdims=True)\n",
    "        # GRADIENT ON INPUTS\n",
    "        self.dinputs=np.dot(dvalues,self.weights.T)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>RELU ACTIVATION : FORWARD AND BACKWARD PASS</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify the original variable,\n",
    "        # let’s make a copy of values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>SOFTMAX ACTIVATION : FORWARD PASS</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOFTMAX ACTIVATION\n",
    "class Activation_Softmax:\n",
    "    # FORWARD PASS \n",
    "    def forward(self, inputs):\n",
    "        # GET UNNORAMLIZED PROBABILITIES\n",
    "        exp_values=np.exp(inputs-np.max(inputs,axis=1,keepdims=True))\n",
    "        # NORMALIZE  THEM FOR EACH SAMPLE\n",
    "        probabilities=exp_values/np.sum(exp_values,axis=1,keepdims=True)\n",
    "\n",
    "        self.output=probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>LOSS</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    # CALCULATE THE DATA AND REGULARIZATION LOSSES\n",
    "    # GIVEN MODEL OUTPUT AND GROUND TRUTH VALUES\n",
    "    def calculate(self,output,y):\n",
    "        # CALCULATE THE SAMPLE LOSS\n",
    "        sample_losses=self.forward(output,y)\n",
    "        # CALCULATE THE MEAN LOSS OVER ALL SAMPLES\n",
    "        data_loss=np.mean(sample_losses)\n",
    "        # RETRUN DATA LOSS\n",
    "        return data_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>CATEGORICAL CROSS ENTROPY LOSS: FORWARD AND BACKWARD PASS</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    # FORWARD PASS\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # NUMBER OF SAMPLES IN A BATCH\n",
    "        samples = len(y_pred)\n",
    "        # CLIP DATA TO PREVENT DIVISION BY ZERO\n",
    "        # CLIP BOTH SIDES TO NOT DRAG TOWARDS ANY VALUE\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1e-7)\n",
    "        # Probailities for target labels\n",
    "        # ONLY IF CATEGORICAL LABELS\n",
    "        if len(y_true.shape) ==1:\n",
    "            correct_confidences=y_pred_clipped[range(samples),y_true]\n",
    "        # MASK VALUE - ONLY FOR ONE HOT ENCODED LABELS\n",
    "        elif len(y_true.shape)==2:\n",
    "            correct_confidences=np.sum(y_pred_clipped*y_true, axis=1)\n",
    "        \n",
    "        neg_log_likelihoods=-np.log(correct_confidences)\n",
    "        return neg_log_likelihoods\n",
    "    def backward(self,davalues,y_true):\n",
    "        # NUMBER OF SAMPLES\n",
    "        samples=len(davalues)\n",
    "        # NUMBER OF LABELS IN EVERY SAMPLE\n",
    "        # WE WILL USE THE FIRST SAMPLE TO COUNT THEM\n",
    "        labels=len(davalues[0])\n",
    "        # IF LABELS ARE SPARSE ,TURN THEM INTO ONE HOT VECTOR\n",
    "        if len(y_true.shape)==1:\n",
    "            y_true=np.eye(labels)[y_true]\n",
    "        \n",
    "        # CALULATE GRADIENT  \n",
    "        self.dinputs=-y_true/davalues\n",
    "        # NORMALIZE GRADIENT \n",
    "        self.dinputs=self.dinputs/samples\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b> COMBINED SOFTMAX ACTIVATION AND CROSS ENTROPY LOSS FOR FASTER BACKWARD STEP</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOFTMAX CLASSIFIER - COMBINED SOFTMAX ACTIVATION\n",
    "# CROSS ENTROPY LOSS FOR FASTER BACKWARD STEP\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "    #CREATES ACTIVATION AND LOSS FUNCTION OBJECTS\n",
    "    def __init__(self):\n",
    "        self.activation=Activation_Softmax()\n",
    "        self.loss=Loss_CategoricalCrossentropy()\n",
    "        \n",
    "    #FORWARD PASS\n",
    "    def forward(self,inputs,y_true):\n",
    "        #OUTPUT LAYERS ACTIVATION FUNCTIONS\n",
    "        self.activation.forward(inputs)\n",
    "        # SET THE OUTPUT\n",
    "        self.output=self.activation.output\n",
    "        # CALCULATE AND RETURN LOSS VALUE\n",
    "        return self.loss.calculate(self.output,y_true)\n",
    "    \n",
    "    #BACKWARD PASS\n",
    "    def backward(self,dvalues,y_true):\n",
    "        # NUMBER OF SAMPLES\n",
    "        samples = len(y_true)\n",
    "        # IF LABELS ARE ONE HOT ENCODED\n",
    "        # TURN THEM INTO DISCRETE VALUE\n",
    "        if len(y_true.shape)==2:\n",
    "            y_true=np.argmax(y_true,axis=1)\n",
    "        # COPY SO WE CAN SAFELY MODIFY\n",
    "        self.dinputs=dvalues.copy()\n",
    "        # CALCULATE GRADIENT\n",
    "        self.dinputs[range(samples),y_true]-=1\n",
    "        # NORMALIZE GRADIENT\n",
    "        self.dinputs=self.dinputs/samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b> FULL CODE UPTO THIS POINT : FORWARD AND BACKWARD PASS</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.3333332  0.3333332  0.33333364]\n",
      " [0.3333329  0.33333293 0.3333342 ]\n",
      " [0.3333326  0.33333263 0.33333477]\n",
      " [0.33333233 0.3333324  0.33333528]]\n",
      "loss: 16.118095\n",
      "[0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 0 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 0 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2]\n",
      "acc: 0.34\n",
      "[[ 1.5766357e-04  7.8368583e-05  4.7324400e-05]\n",
      " [ 1.8161038e-04  1.1045573e-05 -3.3096312e-05]]\n",
      "[[-3.60553473e-04  9.66117223e-05 -1.03671395e-04]]\n",
      "[[ 5.44109462e-05  1.07411419e-04 -1.61822361e-04]\n",
      " [-4.07913431e-05 -7.16780924e-05  1.12469446e-04]\n",
      " [-5.30112993e-05  8.58172934e-05 -3.28059905e-05]]\n",
      "[[-1.0729185e-05 -9.4610732e-06  2.0027859e-05]]\n"
     ]
    }
   ],
   "source": [
    "#CREATE DATASET\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "nnfs.init()\n",
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "# Create second Dense layer with 3 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "# Create Softmax classifier’s combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "# Perform a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "# Perform a forward pass through activation function\n",
    "# takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "# Perform a forward pass through second Dense layer\n",
    "# takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "# Perform a forward pass through the activation/loss function\n",
    "# takes the output of second dense layer here and returns loss\n",
    "loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "# Let’s see output of the first few samples:\n",
    "print(loss_activation.output[:5])\n",
    "# Print loss value\n",
    "print('loss:', loss)\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "# calculate values along first axis\n",
    "predictions = np.argmax(loss_activation.output, axis=1)\n",
    "\n",
    "print(predictions)\n",
    "if len(y.shape) == 2:\n",
    " y = np.argmax(y, axis=1)\n",
    "accuracy = np.mean(predictions == y)\n",
    "# Print accuracy\n",
    "print('acc:', accuracy)\n",
    "# Backward pass\n",
    "loss_activation.backward(loss_activation.output, y)\n",
    "dense2.backward(loss_activation.dinputs)\n",
    "activation1.backward(dense2.dinputs)\n",
    "dense1.backward(activation1.dinputs)\n",
    "# Print gradients\n",
    "print(dense1.dweights)\n",
    "print(dense1.dbiases)\n",
    "print(dense2.dweights)\n",
    "print(dense2.dbiases)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>OPTIMIZERS GRADIENT DESCENT</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDG OPTIMIZER\n",
    "class Optimizer_SGD:\n",
    "    # INITIALIZE OPTIMIZER - SET SETTINGS\n",
    "    # LEARNING RATE OF 1, IS DEFAULT FOR THIS OPTIMIZER\n",
    "    def __init__(self,learning_rate=1):\n",
    "        self.learning_rate=learning_rate\n",
    "    # UPDATE PARAMETERS\n",
    "    def update_params(self,layer):\n",
    "        layer.weights+=-self.learning_rate*layer.dweights\n",
    "        layer.biases+=-self.learning_rate*layer.dbiases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Accuracy: 0.35, Loss: 16.12\n",
      "Epoch: 100, Accuracy: 0.41, Loss: 16.12\n",
      "Epoch: 200, Accuracy: 0.41, Loss: 16.12\n",
      "Epoch: 300, Accuracy: 0.43, Loss: 16.12\n",
      "Epoch: 400, Accuracy: 0.42, Loss: 16.12\n",
      "Epoch: 500, Accuracy: 0.42, Loss: 16.12\n",
      "Epoch: 600, Accuracy: 0.42, Loss: 16.12\n",
      "Epoch: 700, Accuracy: 0.43, Loss: 16.12\n",
      "Epoch: 800, Accuracy: 0.45, Loss: 16.12\n",
      "Epoch: 900, Accuracy: 0.50, Loss: 16.12\n",
      "Epoch: 1000, Accuracy: 0.45, Loss: 16.12\n",
      "Epoch: 1100, Accuracy: 0.46, Loss: 16.12\n",
      "Epoch: 1200, Accuracy: 0.44, Loss: 16.12\n",
      "Epoch: 1300, Accuracy: 0.45, Loss: 16.12\n",
      "Epoch: 1400, Accuracy: 0.42, Loss: 16.12\n",
      "Epoch: 1500, Accuracy: 0.45, Loss: 16.12\n",
      "Epoch: 1600, Accuracy: 0.44, Loss: 16.12\n",
      "Epoch: 1700, Accuracy: 0.49, Loss: 16.12\n",
      "Epoch: 1800, Accuracy: 0.47, Loss: 16.12\n",
      "Epoch: 1900, Accuracy: 0.47, Loss: 16.12\n",
      "Epoch: 2000, Accuracy: 0.50, Loss: 16.12\n",
      "Epoch: 2100, Accuracy: 0.49, Loss: 16.12\n",
      "Epoch: 2200, Accuracy: 0.52, Loss: 16.12\n",
      "Epoch: 2300, Accuracy: 0.52, Loss: 16.12\n",
      "Epoch: 2400, Accuracy: 0.53, Loss: 16.12\n",
      "Epoch: 2500, Accuracy: 0.52, Loss: 16.12\n",
      "Epoch: 2600, Accuracy: 0.54, Loss: 16.12\n",
      "Epoch: 2700, Accuracy: 0.55, Loss: 16.12\n",
      "Epoch: 2800, Accuracy: 0.54, Loss: 16.12\n",
      "Epoch: 2900, Accuracy: 0.58, Loss: 16.12\n",
      "Epoch: 3000, Accuracy: 0.60, Loss: 16.12\n",
      "Epoch: 3100, Accuracy: 0.61, Loss: 16.12\n",
      "Epoch: 3200, Accuracy: 0.57, Loss: 16.12\n",
      "Epoch: 3300, Accuracy: 0.58, Loss: 16.12\n",
      "Epoch: 3400, Accuracy: 0.60, Loss: 16.12\n",
      "Epoch: 3500, Accuracy: 0.61, Loss: 16.12\n",
      "Epoch: 3600, Accuracy: 0.60, Loss: 16.12\n",
      "Epoch: 3700, Accuracy: 0.58, Loss: 16.12\n",
      "Epoch: 3800, Accuracy: 0.60, Loss: 16.12\n",
      "Epoch: 3900, Accuracy: 0.66, Loss: 16.12\n",
      "Epoch: 4000, Accuracy: 0.53, Loss: 16.12\n",
      "Epoch: 4100, Accuracy: 0.62, Loss: 16.12\n",
      "Epoch: 4200, Accuracy: 0.68, Loss: 16.12\n",
      "Epoch: 4300, Accuracy: 0.65, Loss: 16.12\n",
      "Epoch: 4400, Accuracy: 0.61, Loss: 16.12\n",
      "Epoch: 4500, Accuracy: 0.64, Loss: 16.12\n",
      "Epoch: 4600, Accuracy: 0.65, Loss: 16.12\n",
      "Epoch: 4700, Accuracy: 0.64, Loss: 16.12\n",
      "Epoch: 4800, Accuracy: 0.65, Loss: 16.12\n",
      "Epoch: 4900, Accuracy: 0.67, Loss: 16.12\n",
      "Epoch: 5000, Accuracy: 0.68, Loss: 16.12\n",
      "Epoch: 5100, Accuracy: 0.69, Loss: 16.12\n",
      "Epoch: 5200, Accuracy: 0.68, Loss: 16.12\n",
      "Epoch: 5300, Accuracy: 0.68, Loss: 16.12\n",
      "Epoch: 5400, Accuracy: 0.68, Loss: 16.12\n",
      "Epoch: 5500, Accuracy: 0.66, Loss: 16.12\n",
      "Epoch: 5600, Accuracy: 0.70, Loss: 16.12\n",
      "Epoch: 5700, Accuracy: 0.69, Loss: 16.12\n",
      "Epoch: 5800, Accuracy: 0.55, Loss: 16.12\n",
      "Epoch: 5900, Accuracy: 0.70, Loss: 16.12\n",
      "Epoch: 6000, Accuracy: 0.70, Loss: 16.12\n",
      "Epoch: 6100, Accuracy: 0.49, Loss: 16.12\n",
      "Epoch: 6200, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 6300, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 6400, Accuracy: 0.64, Loss: 16.12\n",
      "Epoch: 6500, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 6600, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 6700, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 6800, Accuracy: 0.70, Loss: 16.12\n",
      "Epoch: 6900, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 7000, Accuracy: 0.70, Loss: 16.12\n",
      "Epoch: 7100, Accuracy: 0.70, Loss: 16.12\n",
      "Epoch: 7200, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 7300, Accuracy: 0.69, Loss: 16.12\n",
      "Epoch: 7400, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 7500, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 7600, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 7700, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 7800, Accuracy: 0.70, Loss: 16.12\n",
      "Epoch: 7900, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 8000, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 8100, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 8200, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 8300, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 8400, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 8500, Accuracy: 0.46, Loss: 16.12\n",
      "Epoch: 8600, Accuracy: 0.72, Loss: 16.12\n",
      "Epoch: 8700, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 8800, Accuracy: 0.70, Loss: 16.12\n",
      "Epoch: 8900, Accuracy: 0.72, Loss: 16.12\n",
      "Epoch: 9000, Accuracy: 0.72, Loss: 16.12\n",
      "Epoch: 9100, Accuracy: 0.67, Loss: 16.12\n",
      "Epoch: 9200, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 9300, Accuracy: 0.72, Loss: 16.12\n",
      "Epoch: 9400, Accuracy: 0.73, Loss: 16.12\n",
      "Epoch: 9500, Accuracy: 0.67, Loss: 16.12\n",
      "Epoch: 9600, Accuracy: 0.73, Loss: 16.12\n",
      "Epoch: 9700, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 9800, Accuracy: 0.72, Loss: 16.12\n",
      "Epoch: 9900, Accuracy: 0.73, Loss: 16.12\n"
     ]
    }
   ],
   "source": [
    "# CREATE DATASET\n",
    "X,y=spiral_data(samples=100,classes=3)\n",
    "#CREATE DENSE LAYER WITH @ INPUT FEATURES AND 64 OUTPUT VALUES\n",
    "dense1=Layer_Dense(2,64)\n",
    "#CREATE RELU ACTIVATION FUNCTION\n",
    "activation1=Activation_ReLU()\n",
    "#CREATE DENSE LAYER WITH 64 INPUT FEATURES AND 3 OUTPUT VALUES\n",
    "dense2=Layer_Dense(64,3)\n",
    "#CREATE SOFTMAX ACTIVATION FUNCTION\n",
    "loss_activation=Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "#CREATE OPTIMIZER\n",
    "optimizer=Optimizer_SGD()\n",
    "# TRAINING IN LOOP \n",
    "for epochs in range(10000):\n",
    "    # FORWARD PASS\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    loss=loss_activation.forward(dense2.output,y)\n",
    "\n",
    "    predictions=np.argmax(loss_activation.output,axis=1)\n",
    "    if len(y.shape)==2:\n",
    "        y=np.argmax(y,axis=1)\n",
    "    accuracy=np.mean(predictions==y)\n",
    "\n",
    "    if not epochs %100:\n",
    "        print(f'Epoch: {epochs}, Accuracy: {accuracy:.2f}, Loss: {loss:.2f}')\n",
    "    # BACKWARD PASS\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # UPDATE PARAMS\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
