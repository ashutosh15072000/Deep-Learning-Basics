{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>CREATING LAYERS: FORWARD AND BACKWARD PASS</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DENSE LAYER\n",
    "class Layer_Dense:\n",
    "    # LAYER INITIALIZATION\n",
    "    def __init__(self,n_inputs,n_neurons):\n",
    "        # INITIALIZE WEIGHTS AND BIASES\n",
    "        self.weights=0.01*np.random.randn(n_inputs,n_neurons)\n",
    "        self.biases=np.zeros((1,n_neurons))\n",
    "    # FORWARD PASS\n",
    "    def forward(self,inputs):\n",
    "        # REMEMBER INPUT VALUES\n",
    "        self.inputs=inputs\n",
    "        # CALULATE OUTPUT VALUES FROM INPUTS ONES,WEIGHTS AND BIASES\n",
    "        self.output=np.dot(inputs,self.weights)+self.biases\n",
    "    # BACKWARD PASS\n",
    "    def backward(self, dvalues):\n",
    "        # GRADIENT ON PARAMETERS\n",
    "        self.dweights=np.dot(self.inputs.T,dvalues)\n",
    "        self.dbiases=np.sum(dvalues,axis=0,keepdims=True)\n",
    "        # GRADIENT ON INPUTS\n",
    "        self.dinputs=np.dot(dvalues,self.weights.T)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>RELU ACTIVATION : FORWARD AND BACKWARD PASS</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify the original variable,\n",
    "        # letâ€™s make a copy of values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>SOFTMAX ACTIVATION : FORWARD PASS</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOFTMAX ACTIVATION\n",
    "class Activation_Softmax:\n",
    "    # FORWARD PASS \n",
    "    def forward(self, inputs):\n",
    "        # GET UNNORAMLIZED PROBABILITIES\n",
    "        exp_values=np.exp(inputs-np.max(inputs,axis=1,keepdims=True))\n",
    "        # NORMALIZE  THEM FOR EACH SAMPLE\n",
    "        probabilities=exp_values/np.sum(exp_values,axis=1,keepdims=True)\n",
    "\n",
    "        self.output=probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>LOSS</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    # CALCULATE THE DATA AND REGULARIZATION LOSSES\n",
    "    # GIVEN MODEL OUTPUT AND GROUND TRUTH VALUES\n",
    "    def calculate(self,output,y):\n",
    "        # CALCULATE THE SAMPLE LOSS\n",
    "        sample_losses=self.forward(output,y)\n",
    "        # CALCULATE THE MEAN LOSS OVER ALL SAMPLES\n",
    "        data_loss=np.mean(sample_losses)\n",
    "        # RETRUN DATA LOSS\n",
    "        return data_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>CATEGORICAL CROSS ENTROPY LOSS: FORWARD AND BACKWARD PASS</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    # FORWARD PASS\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # NUMBER OF SAMPLES IN A BATCH\n",
    "        samples = len(y_pred)\n",
    "        # CLIP DATA TO PREVENT DIVISION BY ZERO\n",
    "        # CLIP BOTH SIDES TO NOT DRAG TOWARDS ANY VALUE\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1e-7)\n",
    "        # Probailities for target labels\n",
    "        # ONLY IF CATEGORICAL LABELS\n",
    "        if len(y_true.shape) ==1:\n",
    "            correct_confidences=y_pred_clipped[range(samples),y_true]\n",
    "        # MASK VALUE - ONLY FOR ONE HOT ENCODED LABELS\n",
    "        elif len(y_true.shape)==2:\n",
    "            correct_confidences=np.sum(y_pred_clipped*y_true, axis=1)\n",
    "        \n",
    "        neg_log_likelihoods=-np.log(correct_confidences)\n",
    "        return neg_log_likelihoods\n",
    "    def backward(self,davalues,y_true):\n",
    "        # NUMBER OF SAMPLES\n",
    "        samples=len(davalues)\n",
    "        # NUMBER OF LABELS IN EVERY SAMPLE\n",
    "        # WE WILL USE THE FIRST SAMPLE TO COUNT THEM\n",
    "        labels=len(davalues[0])\n",
    "        # IF LABELS ARE SPARSE ,TURN THEM INTO ONE HOT VECTOR\n",
    "        if len(y_true.shape)==1:\n",
    "            y_true=np.eye(labels)[y_true]\n",
    "        \n",
    "        # CALULATE GRADIENT  \n",
    "        self.dinputs=-y_true/davalues\n",
    "        # NORMALIZE GRADIENT \n",
    "        self.dinputs=self.dinputs/samples\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b> COMBINED SOFTMAX ACTIVATION AND CROSS ENTROPY LOSS FOR FASTER BACKWARD STEP</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOFTMAX CLASSIFIER - COMBINED SOFTMAX ACTIVATION\n",
    "# CROSS ENTROPY LOSS FOR FASTER BACKWARD STEP\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "    #CREATES ACTIVATION AND LOSS FUNCTION OBJECTS\n",
    "    def __init__(self):\n",
    "        self.activation=Activation_Softmax()\n",
    "        self.loss=Loss_CategoricalCrossentropy()\n",
    "        \n",
    "    #FORWARD PASS\n",
    "    def forward(self,inputs,y_true):\n",
    "        #OUTPUT LAYERS ACTIVATION FUNCTIONS\n",
    "        self.activation.forward(inputs)\n",
    "        # SET THE OUTPUT\n",
    "        self.output=self.activation.output\n",
    "        # CALCULATE AND RETURN LOSS VALUE\n",
    "        return self.loss.calculate(self.output,y_true)\n",
    "    \n",
    "    #BACKWARD PASS\n",
    "    def backward(self,dvalues,y_true):\n",
    "        # NUMBER OF SAMPLES\n",
    "        samples = len(y_true)\n",
    "        # IF LABELS ARE ONE HOT ENCODED\n",
    "        # TURN THEM INTO DISCRETE VALUE\n",
    "        if len(y_true.shape)==2:\n",
    "            y_true=np.argmax(y_true,axis=1)\n",
    "        # COPY SO WE CAN SAFELY MODIFY\n",
    "        self.dinputs=dvalues.copy()\n",
    "        # CALCULATE GRADIENT\n",
    "        self.dinputs[range(samples),y_true]-=1\n",
    "        # NORMALIZE GRADIENT\n",
    "        self.dinputs=self.dinputs/samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b> FULL CODE UPTO THIS POINT : FORWARD AND BACKWARD PASS</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.3333332  0.3333332  0.33333364]\n",
      " [0.3333329  0.33333293 0.3333342 ]\n",
      " [0.3333326  0.33333263 0.33333477]\n",
      " [0.33333233 0.3333324  0.33333528]]\n",
      "loss: 16.118095\n",
      "[0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 0 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 0 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2]\n",
      "acc: 0.34\n",
      "[[ 1.5766357e-04  7.8368583e-05  4.7324400e-05]\n",
      " [ 1.8161038e-04  1.1045573e-05 -3.3096312e-05]]\n",
      "[[-3.60553473e-04  9.66117223e-05 -1.03671395e-04]]\n",
      "[[ 5.44109462e-05  1.07411419e-04 -1.61822361e-04]\n",
      " [-4.07913431e-05 -7.16780924e-05  1.12469446e-04]\n",
      " [-5.30112993e-05  8.58172934e-05 -3.28059905e-05]]\n",
      "[[-1.0729185e-05 -9.4610732e-06  2.0027859e-05]]\n"
     ]
    }
   ],
   "source": [
    "#CREATE DATASET\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "nnfs.init()\n",
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "# Create second Dense layer with 3 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "# Create Softmax classifierâ€™s combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "# Perform a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "# Perform a forward pass through activation function\n",
    "# takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "# Perform a forward pass through second Dense layer\n",
    "# takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "# Perform a forward pass through the activation/loss function\n",
    "# takes the output of second dense layer here and returns loss\n",
    "loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "# Letâ€™s see output of the first few samples:\n",
    "print(loss_activation.output[:5])\n",
    "# Print loss value\n",
    "print('loss:', loss)\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "# calculate values along first axis\n",
    "predictions = np.argmax(loss_activation.output, axis=1)\n",
    "\n",
    "print(predictions)\n",
    "if len(y.shape) == 2:\n",
    " y = np.argmax(y, axis=1)\n",
    "accuracy = np.mean(predictions == y)\n",
    "# Print accuracy\n",
    "print('acc:', accuracy)\n",
    "# Backward pass\n",
    "loss_activation.backward(loss_activation.output, y)\n",
    "dense2.backward(loss_activation.dinputs)\n",
    "activation1.backward(dense2.dinputs)\n",
    "dense1.backward(activation1.dinputs)\n",
    "# Print gradients\n",
    "print(dense1.dweights)\n",
    "print(dense1.dbiases)\n",
    "print(dense2.dweights)\n",
    "print(dense2.dbiases)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>OPTIMIZERS GRADIENT DESCENT</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDG OPTIMIZER\n",
    "class Optimizer_SGD:\n",
    "    # INITIALIZE OPTIMIZER - SET SETTINGS\n",
    "    # LEARNING RATE OF 1, IS DEFAULT FOR THIS OPTIMIZER\n",
    "    def __init__(self,learning_rate=1):\n",
    "        self.learning_rate=learning_rate\n",
    "    # UPDATE PARAMETERS\n",
    "    def update_params(self,layer):\n",
    "        layer.weights+=-self.learning_rate*layer.dweights\n",
    "        layer.biases+=-self.learning_rate*layer.dbiases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Accuracy: 0.35, Loss: 16.12\n",
      "Epoch: 100, Accuracy: 0.41, Loss: 16.12\n",
      "Epoch: 200, Accuracy: 0.41, Loss: 16.12\n",
      "Epoch: 300, Accuracy: 0.43, Loss: 16.12\n",
      "Epoch: 400, Accuracy: 0.42, Loss: 16.12\n",
      "Epoch: 500, Accuracy: 0.42, Loss: 16.12\n",
      "Epoch: 600, Accuracy: 0.42, Loss: 16.12\n",
      "Epoch: 700, Accuracy: 0.43, Loss: 16.12\n",
      "Epoch: 800, Accuracy: 0.45, Loss: 16.12\n",
      "Epoch: 900, Accuracy: 0.50, Loss: 16.12\n",
      "Epoch: 1000, Accuracy: 0.45, Loss: 16.12\n",
      "Epoch: 1100, Accuracy: 0.46, Loss: 16.12\n",
      "Epoch: 1200, Accuracy: 0.44, Loss: 16.12\n",
      "Epoch: 1300, Accuracy: 0.45, Loss: 16.12\n",
      "Epoch: 1400, Accuracy: 0.42, Loss: 16.12\n",
      "Epoch: 1500, Accuracy: 0.45, Loss: 16.12\n",
      "Epoch: 1600, Accuracy: 0.44, Loss: 16.12\n",
      "Epoch: 1700, Accuracy: 0.49, Loss: 16.12\n",
      "Epoch: 1800, Accuracy: 0.47, Loss: 16.12\n",
      "Epoch: 1900, Accuracy: 0.47, Loss: 16.12\n",
      "Epoch: 2000, Accuracy: 0.50, Loss: 16.12\n",
      "Epoch: 2100, Accuracy: 0.49, Loss: 16.12\n",
      "Epoch: 2200, Accuracy: 0.52, Loss: 16.12\n",
      "Epoch: 2300, Accuracy: 0.52, Loss: 16.12\n",
      "Epoch: 2400, Accuracy: 0.53, Loss: 16.12\n",
      "Epoch: 2500, Accuracy: 0.52, Loss: 16.12\n",
      "Epoch: 2600, Accuracy: 0.54, Loss: 16.12\n",
      "Epoch: 2700, Accuracy: 0.55, Loss: 16.12\n",
      "Epoch: 2800, Accuracy: 0.54, Loss: 16.12\n",
      "Epoch: 2900, Accuracy: 0.58, Loss: 16.12\n",
      "Epoch: 3000, Accuracy: 0.60, Loss: 16.12\n",
      "Epoch: 3100, Accuracy: 0.61, Loss: 16.12\n",
      "Epoch: 3200, Accuracy: 0.57, Loss: 16.12\n",
      "Epoch: 3300, Accuracy: 0.58, Loss: 16.12\n",
      "Epoch: 3400, Accuracy: 0.60, Loss: 16.12\n",
      "Epoch: 3500, Accuracy: 0.61, Loss: 16.12\n",
      "Epoch: 3600, Accuracy: 0.60, Loss: 16.12\n",
      "Epoch: 3700, Accuracy: 0.58, Loss: 16.12\n",
      "Epoch: 3800, Accuracy: 0.60, Loss: 16.12\n",
      "Epoch: 3900, Accuracy: 0.66, Loss: 16.12\n",
      "Epoch: 4000, Accuracy: 0.53, Loss: 16.12\n",
      "Epoch: 4100, Accuracy: 0.62, Loss: 16.12\n",
      "Epoch: 4200, Accuracy: 0.68, Loss: 16.12\n",
      "Epoch: 4300, Accuracy: 0.65, Loss: 16.12\n",
      "Epoch: 4400, Accuracy: 0.61, Loss: 16.12\n",
      "Epoch: 4500, Accuracy: 0.64, Loss: 16.12\n",
      "Epoch: 4600, Accuracy: 0.65, Loss: 16.12\n",
      "Epoch: 4700, Accuracy: 0.64, Loss: 16.12\n",
      "Epoch: 4800, Accuracy: 0.65, Loss: 16.12\n",
      "Epoch: 4900, Accuracy: 0.67, Loss: 16.12\n",
      "Epoch: 5000, Accuracy: 0.68, Loss: 16.12\n",
      "Epoch: 5100, Accuracy: 0.69, Loss: 16.12\n",
      "Epoch: 5200, Accuracy: 0.68, Loss: 16.12\n",
      "Epoch: 5300, Accuracy: 0.68, Loss: 16.12\n",
      "Epoch: 5400, Accuracy: 0.68, Loss: 16.12\n",
      "Epoch: 5500, Accuracy: 0.66, Loss: 16.12\n",
      "Epoch: 5600, Accuracy: 0.70, Loss: 16.12\n",
      "Epoch: 5700, Accuracy: 0.69, Loss: 16.12\n",
      "Epoch: 5800, Accuracy: 0.55, Loss: 16.12\n",
      "Epoch: 5900, Accuracy: 0.70, Loss: 16.12\n",
      "Epoch: 6000, Accuracy: 0.70, Loss: 16.12\n",
      "Epoch: 6100, Accuracy: 0.49, Loss: 16.12\n",
      "Epoch: 6200, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 6300, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 6400, Accuracy: 0.64, Loss: 16.12\n",
      "Epoch: 6500, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 6600, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 6700, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 6800, Accuracy: 0.70, Loss: 16.12\n",
      "Epoch: 6900, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 7000, Accuracy: 0.70, Loss: 16.12\n",
      "Epoch: 7100, Accuracy: 0.70, Loss: 16.12\n",
      "Epoch: 7200, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 7300, Accuracy: 0.69, Loss: 16.12\n",
      "Epoch: 7400, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 7500, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 7600, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 7700, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 7800, Accuracy: 0.70, Loss: 16.12\n",
      "Epoch: 7900, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 8000, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 8100, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 8200, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 8300, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 8400, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 8500, Accuracy: 0.46, Loss: 16.12\n",
      "Epoch: 8600, Accuracy: 0.72, Loss: 16.12\n",
      "Epoch: 8700, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 8800, Accuracy: 0.70, Loss: 16.12\n",
      "Epoch: 8900, Accuracy: 0.72, Loss: 16.12\n",
      "Epoch: 9000, Accuracy: 0.72, Loss: 16.12\n",
      "Epoch: 9100, Accuracy: 0.67, Loss: 16.12\n",
      "Epoch: 9200, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 9300, Accuracy: 0.72, Loss: 16.12\n",
      "Epoch: 9400, Accuracy: 0.73, Loss: 16.12\n",
      "Epoch: 9500, Accuracy: 0.67, Loss: 16.12\n",
      "Epoch: 9600, Accuracy: 0.73, Loss: 16.12\n",
      "Epoch: 9700, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 9800, Accuracy: 0.72, Loss: 16.12\n",
      "Epoch: 9900, Accuracy: 0.73, Loss: 16.12\n"
     ]
    }
   ],
   "source": [
    "# CREATE DATASET\n",
    "X,y=spiral_data(samples=100,classes=3)\n",
    "#CREATE DENSE LAYER WITH @ INPUT FEATURES AND 64 OUTPUT VALUES\n",
    "dense1=Layer_Dense(2,64)\n",
    "#CREATE RELU ACTIVATION FUNCTION\n",
    "activation1=Activation_ReLU()\n",
    "#CREATE DENSE LAYER WITH 64 INPUT FEATURES AND 3 OUTPUT VALUES\n",
    "dense2=Layer_Dense(64,3)\n",
    "#CREATE SOFTMAX ACTIVATION FUNCTION\n",
    "loss_activation=Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "#CREATE OPTIMIZER\n",
    "optimizer=Optimizer_SGD()\n",
    "# TRAINING IN LOOP \n",
    "for epochs in range(10000):\n",
    "    # FORWARD PASS\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    loss=loss_activation.forward(dense2.output,y)\n",
    "\n",
    "    predictions=np.argmax(loss_activation.output,axis=1)\n",
    "    if len(y.shape)==2:\n",
    "        y=np.argmax(y,axis=1)\n",
    "    accuracy=np.mean(predictions==y)\n",
    "\n",
    "    if not epochs %100:\n",
    "        print(f'Epoch: {epochs}, Accuracy: {accuracy:.2f}, Loss: {loss:.2f}')\n",
    "    # BACKWARD PASS\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # UPDATE PARAMS\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>OPTIMIZERS: LEARNING DECAY</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_SGD:\n",
    "    # INITIAL OTPIMIZER  SET SETTING\n",
    "    # LEARNING RATE OF 1 \n",
    "    def __init__(self,learning_rate=1.0,decay=0.0):\n",
    "        self.learning_rate=learning_rate\n",
    "        self.current_learning_rate=learning_rate\n",
    "        self.decay=decay\n",
    "        self.iterations=0\n",
    "    \n",
    "    # CALL ONCE BEFORE ANY PARAMETER UPDATES\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate=self.learning_rate*(1./(1.+self.decay*self.iterations))\n",
    "    # UPDATE PARAMETERS\n",
    "    def update_params(self,layer):\n",
    "        layer.weights+=-self.current_learning_rate*layer.dweights\n",
    "        layer.biases+=-self.current_learning_rate*layer.dbiases\n",
    "    \n",
    "    # CALL ONCE AFTER ANY PARAMETER UPDATES\n",
    "    def post_update_params(self):\n",
    "        self.iterations+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.340, loss: 16.118, lr: 1.0\n",
      "epoch: 100, acc: 0.420, loss: 16.118, lr: 0.9099181073703367\n",
      "epoch: 200, acc: 0.417, loss: 16.118, lr: 0.8340283569641367\n",
      "epoch: 300, acc: 0.430, loss: 16.118, lr: 0.7698229407236336\n",
      "epoch: 400, acc: 0.423, loss: 16.118, lr: 0.7147962830593281\n",
      "epoch: 500, acc: 0.427, loss: 16.118, lr: 0.66711140760507\n",
      "epoch: 600, acc: 0.430, loss: 16.118, lr: 0.6253908692933083\n",
      "epoch: 700, acc: 0.440, loss: 16.118, lr: 0.5885815185403178\n",
      "epoch: 800, acc: 0.430, loss: 16.118, lr: 0.5558643690939411\n",
      "epoch: 900, acc: 0.430, loss: 16.118, lr: 0.526592943654555\n",
      "epoch: 1000, acc: 0.450, loss: 16.118, lr: 0.5002501250625312\n",
      "epoch: 1100, acc: 0.490, loss: 16.118, lr: 0.4764173415912339\n",
      "epoch: 1200, acc: 0.497, loss: 16.118, lr: 0.45475216007276037\n",
      "epoch: 1300, acc: 0.497, loss: 16.118, lr: 0.43497172683775553\n",
      "epoch: 1400, acc: 0.507, loss: 16.118, lr: 0.4168403501458941\n",
      "epoch: 1500, acc: 0.510, loss: 16.118, lr: 0.4001600640256102\n",
      "epoch: 1600, acc: 0.510, loss: 16.118, lr: 0.3847633705271258\n",
      "epoch: 1700, acc: 0.520, loss: 16.118, lr: 0.3705075954057058\n",
      "epoch: 1800, acc: 0.513, loss: 16.118, lr: 0.35727045373347627\n",
      "epoch: 1900, acc: 0.467, loss: 16.118, lr: 0.3449465332873405\n",
      "epoch: 2000, acc: 0.487, loss: 16.118, lr: 0.33344448149383127\n",
      "epoch: 2100, acc: 0.500, loss: 16.118, lr: 0.32268473701193934\n",
      "epoch: 2200, acc: 0.503, loss: 16.118, lr: 0.31259768677711786\n",
      "epoch: 2300, acc: 0.510, loss: 16.118, lr: 0.3031221582297666\n",
      "epoch: 2400, acc: 0.513, loss: 16.118, lr: 0.29420417769932333\n",
      "epoch: 2500, acc: 0.517, loss: 16.118, lr: 0.2857959416976279\n",
      "epoch: 2600, acc: 0.513, loss: 16.118, lr: 0.2778549597110308\n",
      "epoch: 2700, acc: 0.510, loss: 16.118, lr: 0.2703433360367667\n",
      "epoch: 2800, acc: 0.517, loss: 16.118, lr: 0.26322716504343247\n",
      "epoch: 2900, acc: 0.517, loss: 16.118, lr: 0.25647601949217746\n",
      "epoch: 3000, acc: 0.513, loss: 16.118, lr: 0.25006251562890724\n",
      "epoch: 3100, acc: 0.513, loss: 16.118, lr: 0.2439619419370578\n",
      "epoch: 3200, acc: 0.517, loss: 16.118, lr: 0.23815194093831865\n",
      "epoch: 3300, acc: 0.520, loss: 16.118, lr: 0.23261223540358225\n",
      "epoch: 3400, acc: 0.520, loss: 16.118, lr: 0.22732439190725165\n",
      "epoch: 3500, acc: 0.527, loss: 16.118, lr: 0.22227161591464767\n",
      "epoch: 3600, acc: 0.523, loss: 16.118, lr: 0.21743857360295715\n",
      "epoch: 3700, acc: 0.523, loss: 16.118, lr: 0.21281123643328367\n",
      "epoch: 3800, acc: 0.520, loss: 16.118, lr: 0.20837674515524068\n",
      "epoch: 3900, acc: 0.540, loss: 16.118, lr: 0.20412329046744235\n",
      "epoch: 4000, acc: 0.537, loss: 16.118, lr: 0.2000400080016003\n",
      "epoch: 4100, acc: 0.550, loss: 16.118, lr: 0.19611688566385566\n",
      "epoch: 4200, acc: 0.550, loss: 16.118, lr: 0.19234468166955185\n",
      "epoch: 4300, acc: 0.557, loss: 16.118, lr: 0.18871485185884126\n",
      "epoch: 4400, acc: 0.560, loss: 16.118, lr: 0.18521948508983144\n",
      "epoch: 4500, acc: 0.563, loss: 16.118, lr: 0.18185124568103292\n",
      "epoch: 4600, acc: 0.577, loss: 16.118, lr: 0.1786033220217896\n",
      "epoch: 4700, acc: 0.580, loss: 16.118, lr: 0.1754693805930865\n",
      "epoch: 4800, acc: 0.580, loss: 16.118, lr: 0.17244352474564578\n",
      "epoch: 4900, acc: 0.583, loss: 16.118, lr: 0.16952025767079165\n",
      "epoch: 5000, acc: 0.587, loss: 16.118, lr: 0.16669444907484582\n",
      "epoch: 5100, acc: 0.590, loss: 16.118, lr: 0.16396130513198884\n",
      "epoch: 5200, acc: 0.590, loss: 16.118, lr: 0.16131634134537828\n",
      "epoch: 5300, acc: 0.590, loss: 16.118, lr: 0.15875535799333226\n",
      "epoch: 5400, acc: 0.597, loss: 16.118, lr: 0.1562744178777934\n",
      "epoch: 5500, acc: 0.597, loss: 16.118, lr: 0.15386982612709646\n",
      "epoch: 5600, acc: 0.597, loss: 16.118, lr: 0.15153811183512653\n",
      "epoch: 5700, acc: 0.600, loss: 16.118, lr: 0.14927601134497687\n",
      "epoch: 5800, acc: 0.600, loss: 16.118, lr: 0.14708045300779526\n",
      "epoch: 5900, acc: 0.603, loss: 16.118, lr: 0.14494854326714016\n",
      "epoch: 6000, acc: 0.610, loss: 16.118, lr: 0.1428775539362766\n",
      "epoch: 6100, acc: 0.610, loss: 16.118, lr: 0.1408649105507818\n",
      "epoch: 6200, acc: 0.623, loss: 16.118, lr: 0.13890818169190167\n",
      "epoch: 6300, acc: 0.633, loss: 16.118, lr: 0.13700506918755992\n",
      "epoch: 6400, acc: 0.637, loss: 16.118, lr: 0.13515339910798757\n",
      "epoch: 6500, acc: 0.633, loss: 16.118, lr: 0.13335111348179757\n",
      "epoch: 6600, acc: 0.637, loss: 16.118, lr: 0.13159626266614027\n",
      "epoch: 6700, acc: 0.643, loss: 16.118, lr: 0.12988699831146902\n",
      "epoch: 6800, acc: 0.657, loss: 16.118, lr: 0.12822156686754713\n",
      "epoch: 6900, acc: 0.657, loss: 16.118, lr: 0.126598303582732\n",
      "epoch: 7000, acc: 0.660, loss: 16.118, lr: 0.12501562695336915\n",
      "epoch: 7100, acc: 0.663, loss: 16.118, lr: 0.12347203358439313\n",
      "epoch: 7200, acc: 0.670, loss: 16.118, lr: 0.12196609342602757\n",
      "epoch: 7300, acc: 0.680, loss: 16.118, lr: 0.12049644535486204\n",
      "epoch: 7400, acc: 0.680, loss: 16.118, lr: 0.11906179307060363\n",
      "epoch: 7500, acc: 0.683, loss: 16.118, lr: 0.11766090128250381\n",
      "epoch: 7600, acc: 0.687, loss: 16.118, lr: 0.11629259216187929\n",
      "epoch: 7700, acc: 0.687, loss: 16.118, lr: 0.11495574203931487\n",
      "epoch: 7800, acc: 0.690, loss: 16.118, lr: 0.11364927832708263\n",
      "epoch: 7900, acc: 0.690, loss: 16.118, lr: 0.11237217664906168\n",
      "epoch: 8000, acc: 0.697, loss: 16.118, lr: 0.11112345816201799\n",
      "epoch: 8100, acc: 0.697, loss: 16.118, lr: 0.10990218705352237\n",
      "epoch: 8200, acc: 0.693, loss: 16.118, lr: 0.10870746820306555\n",
      "epoch: 8300, acc: 0.690, loss: 16.118, lr: 0.1075384449940854\n",
      "epoch: 8400, acc: 0.690, loss: 16.118, lr: 0.10639429726566654\n",
      "epoch: 8500, acc: 0.697, loss: 16.118, lr: 0.10527423939362038\n",
      "epoch: 8600, acc: 0.700, loss: 16.118, lr: 0.10417751849150952\n",
      "epoch: 8700, acc: 0.700, loss: 16.118, lr: 0.10310341272296113\n",
      "epoch: 8800, acc: 0.703, loss: 16.118, lr: 0.1020512297173181\n",
      "epoch: 8900, acc: 0.700, loss: 16.118, lr: 0.10102030508132134\n",
      "epoch: 9000, acc: 0.707, loss: 16.118, lr: 0.1000100010001\n",
      "epoch: 9100, acc: 0.707, loss: 16.118, lr: 0.09901970492127933\n",
      "epoch: 9200, acc: 0.700, loss: 16.118, lr: 0.09804882831650162\n",
      "epoch: 9300, acc: 0.700, loss: 16.118, lr: 0.09709680551509856\n",
      "epoch: 9400, acc: 0.700, loss: 16.118, lr: 0.09616309260505818\n",
      "epoch: 9500, acc: 0.697, loss: 16.118, lr: 0.09524716639679968\n",
      "epoch: 9600, acc: 0.697, loss: 16.118, lr: 0.09434852344560807\n",
      "epoch: 9700, acc: 0.697, loss: 16.118, lr: 0.09346667912889055\n",
      "epoch: 9800, acc: 0.697, loss: 16.118, lr: 0.09260116677470137\n",
      "epoch: 9900, acc: 0.700, loss: 16.118, lr: 0.09175153683824203\n",
      "epoch: 10000, acc: 0.707, loss: 16.118, lr: 0.09091735612328393\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming the necessary classes (Layer_Dense, Activation_ReLU, \n",
    "# Activation_Softmax_Loss_CategoricalCrossentropy, and spiral_data) are defined elsewhere\n",
    "\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_SGD(decay=1e-3)\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "    \n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "    \n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "    \n",
    "    # Perform a forward pass through the activation/loss function\n",
    "    # takes the output of second dense layer here and returns loss\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "    \n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # calculate values along first axis\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    \n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {accuracy:.3f}, ' +\n",
    "              f'loss: {loss:.3f}, ' +\n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "    \n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>OPTIMIZERS: MOMENTUM</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Optimizer_SGD:\n",
    "    # INITIALIZE OPTIMIZER SET SETTINGS\n",
    "    # LEARNING RATE OF 1.\n",
    "    def __init__(self,learning_rate=1.0,decay=0.0,momentum=0):\n",
    "        self.learning_rate=learning_rate\n",
    "        self.current_learning_rate=learning_rate\n",
    "        self.decay=decay\n",
    "        self.iterations=0\n",
    "        self.momentum=momentum\n",
    "    # CALL ONCE BEFORE ANY PARAMETER UPDATES\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate=self.learning_rate*(1./(1.+self.decay*self.iterations))\n",
    "    # UPDATE PARAMETER\n",
    "    def update_params(self,layer):\n",
    "        # If WE USE MOMENTUM\n",
    "        if self.momentum:\n",
    "            # IF LAYER DOES NOT CONTAIN MOMENTUM ARRAY,CREATE THEM\n",
    "            # FILLED WITH ZERO\n",
    "            if not hasattr(layer,'weight_momentum'):\n",
    "                layer.weight_momentum=np.zeros_like(layer.weights)\n",
    "                layer.bias_momentum=np.zeros_like(layer.biases)\n",
    "\n",
    "            # BUILD WEIGHT UPDATES WITH MOMENTUM TAKE PREVIOUS\n",
    "            # UPDATES MULTIPLIED BY RETAIN FACTOR AND UPDATE WITH CURRENT\n",
    "\n",
    "            weight_updates=self.momentum*layer.weight_momentum-self.current_learning_rate*layer.dweights\n",
    "            layer.weight_momentum=weight_updates\n",
    "            bias_updates=self.momentum*layer.bias_momentum-self.current_learning_rate*layer.dbiases\n",
    "            layer.bias_momentum=bias_updates\n",
    "        # VANILLA SGD UPDATES\n",
    "        else:\n",
    "            weight_updates=-self.current_learning_rate*layer.dweights\n",
    "            bias_updates=-self.current_learning_rate*layer.dbiases\n",
    "\n",
    "        # UPDATE WEIGHTS AND BAISES USING EITHER VANILLA OR MOMENTUM UPDATES\n",
    "        layer.weights+=weight_updates\n",
    "        layer.biases+=bias_updates\n",
    "   # CALL ONCE AFTER PARAMETER UPDATES\n",
    "    def post_update_params(self):\n",
    "        self.iterations+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.357, loss: 16.118, lr: 1.0\n",
      "epoch: 100, acc: 0.477, loss: 16.118, lr: 0.9099181073703367\n",
      "epoch: 200, acc: 0.623, loss: 16.118, lr: 0.8340283569641367\n",
      "epoch: 300, acc: 0.753, loss: 16.118, lr: 0.7698229407236336\n",
      "epoch: 400, acc: 0.853, loss: 16.118, lr: 0.7147962830593281\n",
      "epoch: 500, acc: 0.890, loss: 16.118, lr: 0.66711140760507\n",
      "epoch: 600, acc: 0.920, loss: 16.118, lr: 0.6253908692933083\n",
      "epoch: 700, acc: 0.927, loss: 16.118, lr: 0.5885815185403178\n",
      "epoch: 800, acc: 0.933, loss: 16.118, lr: 0.5558643690939411\n",
      "epoch: 900, acc: 0.920, loss: 16.118, lr: 0.526592943654555\n",
      "epoch: 1000, acc: 0.917, loss: 16.118, lr: 0.5002501250625312\n",
      "epoch: 1100, acc: 0.943, loss: 16.118, lr: 0.4764173415912339\n",
      "epoch: 1200, acc: 0.943, loss: 16.118, lr: 0.45475216007276037\n",
      "epoch: 1300, acc: 0.950, loss: 16.118, lr: 0.43497172683775553\n",
      "epoch: 1400, acc: 0.950, loss: 16.118, lr: 0.4168403501458941\n",
      "epoch: 1500, acc: 0.950, loss: 16.118, lr: 0.4001600640256102\n",
      "epoch: 1600, acc: 0.950, loss: 16.118, lr: 0.3847633705271258\n",
      "epoch: 1700, acc: 0.950, loss: 16.118, lr: 0.3705075954057058\n",
      "epoch: 1800, acc: 0.947, loss: 16.118, lr: 0.35727045373347627\n",
      "epoch: 1900, acc: 0.950, loss: 16.118, lr: 0.3449465332873405\n",
      "epoch: 2000, acc: 0.947, loss: 16.118, lr: 0.33344448149383127\n",
      "epoch: 2100, acc: 0.950, loss: 16.118, lr: 0.32268473701193934\n",
      "epoch: 2200, acc: 0.947, loss: 16.118, lr: 0.31259768677711786\n",
      "epoch: 2300, acc: 0.950, loss: 16.118, lr: 0.3031221582297666\n",
      "epoch: 2400, acc: 0.950, loss: 16.118, lr: 0.29420417769932333\n",
      "epoch: 2500, acc: 0.950, loss: 16.118, lr: 0.2857959416976279\n",
      "epoch: 2600, acc: 0.950, loss: 16.118, lr: 0.2778549597110308\n",
      "epoch: 2700, acc: 0.950, loss: 16.118, lr: 0.2703433360367667\n",
      "epoch: 2800, acc: 0.950, loss: 16.118, lr: 0.26322716504343247\n",
      "epoch: 2900, acc: 0.950, loss: 16.118, lr: 0.25647601949217746\n",
      "epoch: 3000, acc: 0.953, loss: 16.118, lr: 0.25006251562890724\n",
      "epoch: 3100, acc: 0.953, loss: 16.118, lr: 0.2439619419370578\n",
      "epoch: 3200, acc: 0.953, loss: 16.118, lr: 0.23815194093831865\n",
      "epoch: 3300, acc: 0.953, loss: 16.118, lr: 0.23261223540358225\n",
      "epoch: 3400, acc: 0.953, loss: 16.118, lr: 0.22732439190725165\n",
      "epoch: 3500, acc: 0.953, loss: 16.118, lr: 0.22227161591464767\n",
      "epoch: 3600, acc: 0.953, loss: 16.118, lr: 0.21743857360295715\n",
      "epoch: 3700, acc: 0.953, loss: 16.118, lr: 0.21281123643328367\n",
      "epoch: 3800, acc: 0.953, loss: 16.118, lr: 0.20837674515524068\n",
      "epoch: 3900, acc: 0.953, loss: 16.118, lr: 0.20412329046744235\n",
      "epoch: 4000, acc: 0.953, loss: 16.118, lr: 0.2000400080016003\n",
      "epoch: 4100, acc: 0.953, loss: 16.118, lr: 0.19611688566385566\n",
      "epoch: 4200, acc: 0.953, loss: 16.118, lr: 0.19234468166955185\n",
      "epoch: 4300, acc: 0.950, loss: 16.118, lr: 0.18871485185884126\n",
      "epoch: 4400, acc: 0.953, loss: 16.118, lr: 0.18521948508983144\n",
      "epoch: 4500, acc: 0.953, loss: 16.118, lr: 0.18185124568103292\n",
      "epoch: 4600, acc: 0.950, loss: 16.118, lr: 0.1786033220217896\n",
      "epoch: 4700, acc: 0.950, loss: 16.118, lr: 0.1754693805930865\n",
      "epoch: 4800, acc: 0.950, loss: 16.118, lr: 0.17244352474564578\n",
      "epoch: 4900, acc: 0.950, loss: 16.118, lr: 0.16952025767079165\n",
      "epoch: 5000, acc: 0.953, loss: 16.118, lr: 0.16669444907484582\n",
      "epoch: 5100, acc: 0.950, loss: 16.118, lr: 0.16396130513198884\n",
      "epoch: 5200, acc: 0.950, loss: 16.118, lr: 0.16131634134537828\n",
      "epoch: 5300, acc: 0.950, loss: 16.118, lr: 0.15875535799333226\n",
      "epoch: 5400, acc: 0.947, loss: 16.118, lr: 0.1562744178777934\n",
      "epoch: 5500, acc: 0.947, loss: 16.118, lr: 0.15386982612709646\n",
      "epoch: 5600, acc: 0.947, loss: 16.118, lr: 0.15153811183512653\n",
      "epoch: 5700, acc: 0.947, loss: 16.118, lr: 0.14927601134497687\n",
      "epoch: 5800, acc: 0.947, loss: 16.118, lr: 0.14708045300779526\n",
      "epoch: 5900, acc: 0.947, loss: 16.118, lr: 0.14494854326714016\n",
      "epoch: 6000, acc: 0.947, loss: 16.118, lr: 0.1428775539362766\n",
      "epoch: 6100, acc: 0.947, loss: 16.118, lr: 0.1408649105507818\n",
      "epoch: 6200, acc: 0.947, loss: 16.118, lr: 0.13890818169190167\n",
      "epoch: 6300, acc: 0.947, loss: 16.118, lr: 0.13700506918755992\n",
      "epoch: 6400, acc: 0.947, loss: 16.118, lr: 0.13515339910798757\n",
      "epoch: 6500, acc: 0.947, loss: 16.118, lr: 0.13335111348179757\n",
      "epoch: 6600, acc: 0.947, loss: 16.118, lr: 0.13159626266614027\n",
      "epoch: 6700, acc: 0.947, loss: 16.118, lr: 0.12988699831146902\n",
      "epoch: 6800, acc: 0.947, loss: 16.118, lr: 0.12822156686754713\n",
      "epoch: 6900, acc: 0.947, loss: 16.118, lr: 0.126598303582732\n",
      "epoch: 7000, acc: 0.947, loss: 16.118, lr: 0.12501562695336915\n",
      "epoch: 7100, acc: 0.947, loss: 16.118, lr: 0.12347203358439313\n",
      "epoch: 7200, acc: 0.947, loss: 16.118, lr: 0.12196609342602757\n",
      "epoch: 7300, acc: 0.947, loss: 16.118, lr: 0.12049644535486204\n",
      "epoch: 7400, acc: 0.947, loss: 16.118, lr: 0.11906179307060363\n",
      "epoch: 7500, acc: 0.947, loss: 16.118, lr: 0.11766090128250381\n",
      "epoch: 7600, acc: 0.947, loss: 16.118, lr: 0.11629259216187929\n",
      "epoch: 7700, acc: 0.947, loss: 16.118, lr: 0.11495574203931487\n",
      "epoch: 7800, acc: 0.947, loss: 16.118, lr: 0.11364927832708263\n",
      "epoch: 7900, acc: 0.947, loss: 16.118, lr: 0.11237217664906168\n",
      "epoch: 8000, acc: 0.947, loss: 16.118, lr: 0.11112345816201799\n",
      "epoch: 8100, acc: 0.950, loss: 16.118, lr: 0.10990218705352237\n",
      "epoch: 8200, acc: 0.950, loss: 16.118, lr: 0.10870746820306555\n",
      "epoch: 8300, acc: 0.950, loss: 16.118, lr: 0.1075384449940854\n",
      "epoch: 8400, acc: 0.950, loss: 16.118, lr: 0.10639429726566654\n",
      "epoch: 8500, acc: 0.950, loss: 16.118, lr: 0.10527423939362038\n",
      "epoch: 8600, acc: 0.950, loss: 16.118, lr: 0.10417751849150952\n",
      "epoch: 8700, acc: 0.950, loss: 16.118, lr: 0.10310341272296113\n",
      "epoch: 8800, acc: 0.950, loss: 16.118, lr: 0.1020512297173181\n",
      "epoch: 8900, acc: 0.950, loss: 16.118, lr: 0.10102030508132134\n",
      "epoch: 9000, acc: 0.950, loss: 16.118, lr: 0.1000100010001\n",
      "epoch: 9100, acc: 0.950, loss: 16.118, lr: 0.09901970492127933\n",
      "epoch: 9200, acc: 0.953, loss: 16.118, lr: 0.09804882831650162\n",
      "epoch: 9300, acc: 0.950, loss: 16.118, lr: 0.09709680551509856\n",
      "epoch: 9400, acc: 0.950, loss: 16.118, lr: 0.09616309260505818\n",
      "epoch: 9500, acc: 0.953, loss: 16.118, lr: 0.09524716639679968\n",
      "epoch: 9600, acc: 0.953, loss: 16.118, lr: 0.09434852344560807\n",
      "epoch: 9700, acc: 0.953, loss: 16.118, lr: 0.09346667912889055\n",
      "epoch: 9800, acc: 0.953, loss: 16.118, lr: 0.09260116677470137\n",
      "epoch: 9900, acc: 0.953, loss: 16.118, lr: 0.09175153683824203\n",
      "epoch: 10000, acc: 0.953, loss: 16.118, lr: 0.09091735612328393\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming the necessary classes (Layer_Dense, Activation_ReLU, \n",
    "# Activation_Softmax_Loss_CategoricalCrossentropy, Optimizer_SGD, and spiral_data) are defined elsewhere\n",
    "\n",
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_SGD(decay=1e-3, momentum=0.9)\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "    \n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "    \n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "    \n",
    "    # Perform a forward pass through the activation/loss function\n",
    "    # takes the output of second dense layer here and returns loss\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "    \n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # calculate values along first axis\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    \n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {accuracy:.3f}, ' +\n",
    "              f'loss: {loss:.3f}, ' +\n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "    \n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
