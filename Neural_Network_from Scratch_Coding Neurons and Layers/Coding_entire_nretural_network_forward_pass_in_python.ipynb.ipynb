{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>CREATING LAYERS: FORWARD AND BACKWARD PASS</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DENSE LAYER\n",
    "class Layer_Dense:\n",
    "    # LAYER INITIALIZATION\n",
    "    def __init__(self,n_inputs,n_neurons):\n",
    "        # INITIALIZE WEIGHTS AND BIASES\n",
    "        self.weights=0.01*np.random.randn(n_inputs,n_neurons)\n",
    "        self.biases=np.zeros((1,n_neurons))\n",
    "    # FORWARD PASS\n",
    "    def forward(self,inputs):\n",
    "        # REMEMBER INPUT VALUES\n",
    "        self.inputs=inputs\n",
    "        # CALULATE OUTPUT VALUES FROM INPUTS ONES,WEIGHTS AND BIASES\n",
    "        self.output=np.dot(inputs,self.weights)+self.biases\n",
    "    # BACKWARD PASS\n",
    "    def backward(self, dvalues):\n",
    "        # GRADIENT ON PARAMETERS\n",
    "        self.dweights=np.dot(self.inputs.T,dvalues)\n",
    "        self.dbiases=np.sum(dvalues,axis=0,keepdims=True)\n",
    "        # GRADIENT ON INPUTS\n",
    "        self.dinputs=np.dot(dvalues,self.weights.T)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>RELU ACTIVATION : FORWARD AND BACKWARD PASS</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify the original variable,\n",
    "        # let’s make a copy of values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>SOFTMAX ACTIVATION : FORWARD PASS</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOFTMAX ACTIVATION\n",
    "class Activation_Softmax:\n",
    "    # FORWARD PASS \n",
    "    def forward(self, inputs):\n",
    "        # GET UNNORAMLIZED PROBABILITIES\n",
    "        exp_values=np.exp(inputs-np.max(inputs,axis=1,keepdims=True))\n",
    "        # NORMALIZE  THEM FOR EACH SAMPLE\n",
    "        probabilities=exp_values/np.sum(exp_values,axis=1,keepdims=True)\n",
    "\n",
    "        self.output=probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>LOSS</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    # CALCULATE THE DATA AND REGULARIZATION LOSSES\n",
    "    # GIVEN MODEL OUTPUT AND GROUND TRUTH VALUES\n",
    "    def calculate(self,output,y):\n",
    "        # CALCULATE THE SAMPLE LOSS\n",
    "        sample_losses=self.forward(output,y)\n",
    "        # CALCULATE THE MEAN LOSS OVER ALL SAMPLES\n",
    "        data_loss=np.mean(sample_losses)\n",
    "        # RETRUN DATA LOSS\n",
    "        return data_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>CATEGORICAL CROSS ENTROPY LOSS: FORWARD AND BACKWARD PASS</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    # FORWARD PASS\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # NUMBER OF SAMPLES IN A BATCH\n",
    "        samples = len(y_pred)\n",
    "        # CLIP DATA TO PREVENT DIVISION BY ZERO\n",
    "        # CLIP BOTH SIDES TO NOT DRAG TOWARDS ANY VALUE\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1e-7)\n",
    "        # Probailities for target labels\n",
    "        # ONLY IF CATEGORICAL LABELS\n",
    "        if len(y_true.shape) ==1:\n",
    "            correct_confidences=y_pred_clipped[range(samples),y_true]\n",
    "        # MASK VALUE - ONLY FOR ONE HOT ENCODED LABELS\n",
    "        elif len(y_true.shape)==2:\n",
    "            correct_confidences=np.sum(y_pred_clipped*y_true, axis=1)\n",
    "        \n",
    "        neg_log_likelihoods=-np.log(correct_confidences)\n",
    "        return neg_log_likelihoods\n",
    "    def backward(self,davalues,y_true):\n",
    "        # NUMBER OF SAMPLES\n",
    "        samples=len(davalues)\n",
    "        # NUMBER OF LABELS IN EVERY SAMPLE\n",
    "        # WE WILL USE THE FIRST SAMPLE TO COUNT THEM\n",
    "        labels=len(davalues[0])\n",
    "        # IF LABELS ARE SPARSE ,TURN THEM INTO ONE HOT VECTOR\n",
    "        if len(y_true.shape)==1:\n",
    "            y_true=np.eye(labels)[y_true]\n",
    "        \n",
    "        # CALULATE GRADIENT  \n",
    "        self.dinputs=-y_true/davalues\n",
    "        # NORMALIZE GRADIENT \n",
    "        self.dinputs=self.dinputs/samples\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b> COMBINED SOFTMAX ACTIVATION AND CROSS ENTROPY LOSS FOR FASTER BACKWARD STEP</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOFTMAX CLASSIFIER - COMBINED SOFTMAX ACTIVATION\n",
    "# CROSS ENTROPY LOSS FOR FASTER BACKWARD STEP\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "    #CREATES ACTIVATION AND LOSS FUNCTION OBJECTS\n",
    "    def __init__(self):\n",
    "        self.activation=Activation_Softmax()\n",
    "        self.loss=Loss_CategoricalCrossentropy()\n",
    "        \n",
    "    #FORWARD PASS\n",
    "    def forward(self,inputs,y_true):\n",
    "        #OUTPUT LAYERS ACTIVATION FUNCTIONS\n",
    "        self.activation.forward(inputs)\n",
    "        # SET THE OUTPUT\n",
    "        self.output=self.activation.output\n",
    "        # CALCULATE AND RETURN LOSS VALUE\n",
    "        return self.loss.calculate(self.output,y_true)\n",
    "    \n",
    "    #BACKWARD PASS\n",
    "    def backward(self,dvalues,y_true):\n",
    "        # NUMBER OF SAMPLES\n",
    "        samples = len(y_true)\n",
    "        # IF LABELS ARE ONE HOT ENCODED\n",
    "        # TURN THEM INTO DISCRETE VALUE\n",
    "        if len(y_true.shape)==2:\n",
    "            y_true=np.argmax(y_true,axis=1)\n",
    "        # COPY SO WE CAN SAFELY MODIFY\n",
    "        self.dinputs=dvalues.copy()\n",
    "        # CALCULATE GRADIENT\n",
    "        self.dinputs[range(samples),y_true]-=1\n",
    "        # NORMALIZE GRADIENT\n",
    "        self.dinputs=self.dinputs/samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b> FULL CODE UPTO THIS POINT : FORWARD AND BACKWARD PASS</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.3333332  0.3333332  0.33333364]\n",
      " [0.3333329  0.33333293 0.3333342 ]\n",
      " [0.3333326  0.33333263 0.33333477]\n",
      " [0.33333233 0.3333324  0.33333528]]\n",
      "loss: 16.118095\n",
      "[0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 0 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 0 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2]\n",
      "acc: 0.34\n",
      "[[ 1.5766357e-04  7.8368583e-05  4.7324400e-05]\n",
      " [ 1.8161038e-04  1.1045573e-05 -3.3096312e-05]]\n",
      "[[-3.60553473e-04  9.66117223e-05 -1.03671395e-04]]\n",
      "[[ 5.44109462e-05  1.07411419e-04 -1.61822361e-04]\n",
      " [-4.07913431e-05 -7.16780924e-05  1.12469446e-04]\n",
      " [-5.30112993e-05  8.58172934e-05 -3.28059905e-05]]\n",
      "[[-1.0729185e-05 -9.4610732e-06  2.0027859e-05]]\n"
     ]
    }
   ],
   "source": [
    "#CREATE DATASET\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "nnfs.init()\n",
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "# Create second Dense layer with 3 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "# Create Softmax classifier’s combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "# Perform a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "# Perform a forward pass through activation function\n",
    "# takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "# Perform a forward pass through second Dense layer\n",
    "# takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "# Perform a forward pass through the activation/loss function\n",
    "# takes the output of second dense layer here and returns loss\n",
    "loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "# Let’s see output of the first few samples:\n",
    "print(loss_activation.output[:5])\n",
    "# Print loss value\n",
    "print('loss:', loss)\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "# calculate values along first axis\n",
    "predictions = np.argmax(loss_activation.output, axis=1)\n",
    "\n",
    "print(predictions)\n",
    "if len(y.shape) == 2:\n",
    " y = np.argmax(y, axis=1)\n",
    "accuracy = np.mean(predictions == y)\n",
    "# Print accuracy\n",
    "print('acc:', accuracy)\n",
    "# Backward pass\n",
    "loss_activation.backward(loss_activation.output, y)\n",
    "dense2.backward(loss_activation.dinputs)\n",
    "activation1.backward(dense2.dinputs)\n",
    "dense1.backward(activation1.dinputs)\n",
    "# Print gradients\n",
    "print(dense1.dweights)\n",
    "print(dense1.dbiases)\n",
    "print(dense2.dweights)\n",
    "print(dense2.dbiases)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>OPTIMIZERS GRADIENT DESCENT</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDG OPTIMIZER\n",
    "class Optimizer_SGD:\n",
    "    # INITIALIZE OPTIMIZER - SET SETTINGS\n",
    "    # LEARNING RATE OF 1, IS DEFAULT FOR THIS OPTIMIZER\n",
    "    def __init__(self,learning_rate=1):\n",
    "        self.learning_rate=learning_rate\n",
    "    # UPDATE PARAMETERS\n",
    "    def update_params(self,layer):\n",
    "        layer.weights+=-self.learning_rate*layer.dweights\n",
    "        layer.biases+=-self.learning_rate*layer.dbiases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Accuracy: 0.35, Loss: 16.12\n",
      "Epoch: 100, Accuracy: 0.41, Loss: 16.12\n",
      "Epoch: 200, Accuracy: 0.41, Loss: 16.12\n",
      "Epoch: 300, Accuracy: 0.43, Loss: 16.12\n",
      "Epoch: 400, Accuracy: 0.42, Loss: 16.12\n",
      "Epoch: 500, Accuracy: 0.42, Loss: 16.12\n",
      "Epoch: 600, Accuracy: 0.42, Loss: 16.12\n",
      "Epoch: 700, Accuracy: 0.43, Loss: 16.12\n",
      "Epoch: 800, Accuracy: 0.45, Loss: 16.12\n",
      "Epoch: 900, Accuracy: 0.50, Loss: 16.12\n",
      "Epoch: 1000, Accuracy: 0.45, Loss: 16.12\n",
      "Epoch: 1100, Accuracy: 0.46, Loss: 16.12\n",
      "Epoch: 1200, Accuracy: 0.44, Loss: 16.12\n",
      "Epoch: 1300, Accuracy: 0.45, Loss: 16.12\n",
      "Epoch: 1400, Accuracy: 0.42, Loss: 16.12\n",
      "Epoch: 1500, Accuracy: 0.45, Loss: 16.12\n",
      "Epoch: 1600, Accuracy: 0.44, Loss: 16.12\n",
      "Epoch: 1700, Accuracy: 0.49, Loss: 16.12\n",
      "Epoch: 1800, Accuracy: 0.47, Loss: 16.12\n",
      "Epoch: 1900, Accuracy: 0.47, Loss: 16.12\n",
      "Epoch: 2000, Accuracy: 0.50, Loss: 16.12\n",
      "Epoch: 2100, Accuracy: 0.49, Loss: 16.12\n",
      "Epoch: 2200, Accuracy: 0.52, Loss: 16.12\n",
      "Epoch: 2300, Accuracy: 0.52, Loss: 16.12\n",
      "Epoch: 2400, Accuracy: 0.53, Loss: 16.12\n",
      "Epoch: 2500, Accuracy: 0.52, Loss: 16.12\n",
      "Epoch: 2600, Accuracy: 0.54, Loss: 16.12\n",
      "Epoch: 2700, Accuracy: 0.55, Loss: 16.12\n",
      "Epoch: 2800, Accuracy: 0.54, Loss: 16.12\n",
      "Epoch: 2900, Accuracy: 0.58, Loss: 16.12\n",
      "Epoch: 3000, Accuracy: 0.60, Loss: 16.12\n",
      "Epoch: 3100, Accuracy: 0.61, Loss: 16.12\n",
      "Epoch: 3200, Accuracy: 0.57, Loss: 16.12\n",
      "Epoch: 3300, Accuracy: 0.58, Loss: 16.12\n",
      "Epoch: 3400, Accuracy: 0.60, Loss: 16.12\n",
      "Epoch: 3500, Accuracy: 0.61, Loss: 16.12\n",
      "Epoch: 3600, Accuracy: 0.60, Loss: 16.12\n",
      "Epoch: 3700, Accuracy: 0.58, Loss: 16.12\n",
      "Epoch: 3800, Accuracy: 0.60, Loss: 16.12\n",
      "Epoch: 3900, Accuracy: 0.66, Loss: 16.12\n",
      "Epoch: 4000, Accuracy: 0.53, Loss: 16.12\n",
      "Epoch: 4100, Accuracy: 0.62, Loss: 16.12\n",
      "Epoch: 4200, Accuracy: 0.68, Loss: 16.12\n",
      "Epoch: 4300, Accuracy: 0.65, Loss: 16.12\n",
      "Epoch: 4400, Accuracy: 0.61, Loss: 16.12\n",
      "Epoch: 4500, Accuracy: 0.64, Loss: 16.12\n",
      "Epoch: 4600, Accuracy: 0.65, Loss: 16.12\n",
      "Epoch: 4700, Accuracy: 0.64, Loss: 16.12\n",
      "Epoch: 4800, Accuracy: 0.65, Loss: 16.12\n",
      "Epoch: 4900, Accuracy: 0.67, Loss: 16.12\n",
      "Epoch: 5000, Accuracy: 0.68, Loss: 16.12\n",
      "Epoch: 5100, Accuracy: 0.69, Loss: 16.12\n",
      "Epoch: 5200, Accuracy: 0.68, Loss: 16.12\n",
      "Epoch: 5300, Accuracy: 0.68, Loss: 16.12\n",
      "Epoch: 5400, Accuracy: 0.68, Loss: 16.12\n",
      "Epoch: 5500, Accuracy: 0.66, Loss: 16.12\n",
      "Epoch: 5600, Accuracy: 0.70, Loss: 16.12\n",
      "Epoch: 5700, Accuracy: 0.69, Loss: 16.12\n",
      "Epoch: 5800, Accuracy: 0.55, Loss: 16.12\n",
      "Epoch: 5900, Accuracy: 0.70, Loss: 16.12\n",
      "Epoch: 6000, Accuracy: 0.70, Loss: 16.12\n",
      "Epoch: 6100, Accuracy: 0.49, Loss: 16.12\n",
      "Epoch: 6200, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 6300, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 6400, Accuracy: 0.64, Loss: 16.12\n",
      "Epoch: 6500, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 6600, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 6700, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 6800, Accuracy: 0.70, Loss: 16.12\n",
      "Epoch: 6900, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 7000, Accuracy: 0.70, Loss: 16.12\n",
      "Epoch: 7100, Accuracy: 0.70, Loss: 16.12\n",
      "Epoch: 7200, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 7300, Accuracy: 0.69, Loss: 16.12\n",
      "Epoch: 7400, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 7500, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 7600, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 7700, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 7800, Accuracy: 0.70, Loss: 16.12\n",
      "Epoch: 7900, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 8000, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 8100, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 8200, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 8300, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 8400, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 8500, Accuracy: 0.46, Loss: 16.12\n",
      "Epoch: 8600, Accuracy: 0.72, Loss: 16.12\n",
      "Epoch: 8700, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 8800, Accuracy: 0.70, Loss: 16.12\n",
      "Epoch: 8900, Accuracy: 0.72, Loss: 16.12\n",
      "Epoch: 9000, Accuracy: 0.72, Loss: 16.12\n",
      "Epoch: 9100, Accuracy: 0.67, Loss: 16.12\n",
      "Epoch: 9200, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 9300, Accuracy: 0.72, Loss: 16.12\n",
      "Epoch: 9400, Accuracy: 0.73, Loss: 16.12\n",
      "Epoch: 9500, Accuracy: 0.67, Loss: 16.12\n",
      "Epoch: 9600, Accuracy: 0.73, Loss: 16.12\n",
      "Epoch: 9700, Accuracy: 0.71, Loss: 16.12\n",
      "Epoch: 9800, Accuracy: 0.72, Loss: 16.12\n",
      "Epoch: 9900, Accuracy: 0.73, Loss: 16.12\n"
     ]
    }
   ],
   "source": [
    "# CREATE DATASET\n",
    "X,y=spiral_data(samples=100,classes=3)\n",
    "#CREATE DENSE LAYER WITH @ INPUT FEATURES AND 64 OUTPUT VALUES\n",
    "dense1=Layer_Dense(2,64)\n",
    "#CREATE RELU ACTIVATION FUNCTION\n",
    "activation1=Activation_ReLU()\n",
    "#CREATE DENSE LAYER WITH 64 INPUT FEATURES AND 3 OUTPUT VALUES\n",
    "dense2=Layer_Dense(64,3)\n",
    "#CREATE SOFTMAX ACTIVATION FUNCTION\n",
    "loss_activation=Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "#CREATE OPTIMIZER\n",
    "optimizer=Optimizer_SGD()\n",
    "# TRAINING IN LOOP \n",
    "for epochs in range(10000):\n",
    "    # FORWARD PASS\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    loss=loss_activation.forward(dense2.output,y)\n",
    "\n",
    "    predictions=np.argmax(loss_activation.output,axis=1)\n",
    "    if len(y.shape)==2:\n",
    "        y=np.argmax(y,axis=1)\n",
    "    accuracy=np.mean(predictions==y)\n",
    "\n",
    "    if not epochs %100:\n",
    "        print(f'Epoch: {epochs}, Accuracy: {accuracy:.2f}, Loss: {loss:.2f}')\n",
    "    # BACKWARD PASS\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # UPDATE PARAMS\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>OPTIMIZERS: LEARNING DECAY</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_SGD:\n",
    "    # INITIAL OTPIMIZER  SET SETTING\n",
    "    # LEARNING RATE OF 1 \n",
    "    def __init__(self,learning_rate=1.0,decay=0.0):\n",
    "        self.learning_rate=learning_rate\n",
    "        self.current_learning_rate=learning_rate\n",
    "        self.decay=decay\n",
    "        self.iterations=0\n",
    "    \n",
    "    # CALL ONCE BEFORE ANY PARAMETER UPDATES\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate=self.learning_rate*(1./(1.+self.decay*self.iterations))\n",
    "    # UPDATE PARAMETERS\n",
    "    def update_params(self,layer):\n",
    "        layer.weights+=-self.current_learning_rate*layer.dweights\n",
    "        layer.biases+=-self.current_learning_rate*layer.dbiases\n",
    "    \n",
    "    # CALL ONCE AFTER ANY PARAMETER UPDATES\n",
    "    def post_update_params(self):\n",
    "        self.iterations+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.367, loss: 16.118, lr: 1.0\n",
      "epoch: 100, acc: 0.410, loss: 16.118, lr: 0.9099181073703367\n",
      "epoch: 200, acc: 0.407, loss: 16.118, lr: 0.8340283569641367\n",
      "epoch: 300, acc: 0.410, loss: 16.118, lr: 0.7698229407236336\n",
      "epoch: 400, acc: 0.417, loss: 16.118, lr: 0.7147962830593281\n",
      "epoch: 500, acc: 0.413, loss: 16.118, lr: 0.66711140760507\n",
      "epoch: 600, acc: 0.423, loss: 16.118, lr: 0.6253908692933083\n",
      "epoch: 700, acc: 0.417, loss: 16.118, lr: 0.5885815185403178\n",
      "epoch: 800, acc: 0.423, loss: 16.118, lr: 0.5558643690939411\n",
      "epoch: 900, acc: 0.410, loss: 16.118, lr: 0.526592943654555\n",
      "epoch: 1000, acc: 0.413, loss: 16.118, lr: 0.5002501250625312\n",
      "epoch: 1100, acc: 0.423, loss: 16.118, lr: 0.4764173415912339\n",
      "epoch: 1200, acc: 0.430, loss: 16.118, lr: 0.45475216007276037\n",
      "epoch: 1300, acc: 0.437, loss: 16.118, lr: 0.43497172683775553\n",
      "epoch: 1400, acc: 0.450, loss: 16.118, lr: 0.4168403501458941\n",
      "epoch: 1500, acc: 0.457, loss: 16.118, lr: 0.4001600640256102\n",
      "epoch: 1600, acc: 0.460, loss: 16.118, lr: 0.3847633705271258\n",
      "epoch: 1700, acc: 0.457, loss: 16.118, lr: 0.3705075954057058\n",
      "epoch: 1800, acc: 0.467, loss: 16.118, lr: 0.35727045373347627\n",
      "epoch: 1900, acc: 0.487, loss: 16.118, lr: 0.3449465332873405\n",
      "epoch: 2000, acc: 0.513, loss: 16.118, lr: 0.33344448149383127\n",
      "epoch: 2100, acc: 0.520, loss: 16.118, lr: 0.32268473701193934\n",
      "epoch: 2200, acc: 0.523, loss: 16.118, lr: 0.31259768677711786\n",
      "epoch: 2300, acc: 0.527, loss: 16.118, lr: 0.3031221582297666\n",
      "epoch: 2400, acc: 0.540, loss: 16.118, lr: 0.29420417769932333\n",
      "epoch: 2500, acc: 0.547, loss: 16.118, lr: 0.2857959416976279\n",
      "epoch: 2600, acc: 0.530, loss: 16.118, lr: 0.2778549597110308\n",
      "epoch: 2700, acc: 0.573, loss: 16.118, lr: 0.2703433360367667\n",
      "epoch: 2800, acc: 0.563, loss: 16.118, lr: 0.26322716504343247\n",
      "epoch: 2900, acc: 0.563, loss: 16.118, lr: 0.25647601949217746\n",
      "epoch: 3000, acc: 0.570, loss: 16.118, lr: 0.25006251562890724\n",
      "epoch: 3100, acc: 0.560, loss: 16.118, lr: 0.2439619419370578\n",
      "epoch: 3200, acc: 0.563, loss: 16.118, lr: 0.23815194093831865\n",
      "epoch: 3300, acc: 0.537, loss: 16.118, lr: 0.23261223540358225\n",
      "epoch: 3400, acc: 0.507, loss: 16.118, lr: 0.22732439190725165\n",
      "epoch: 3500, acc: 0.507, loss: 16.118, lr: 0.22227161591464767\n",
      "epoch: 3600, acc: 0.517, loss: 16.118, lr: 0.21743857360295715\n",
      "epoch: 3700, acc: 0.517, loss: 16.118, lr: 0.21281123643328367\n",
      "epoch: 3800, acc: 0.510, loss: 16.118, lr: 0.20837674515524068\n",
      "epoch: 3900, acc: 0.507, loss: 16.118, lr: 0.20412329046744235\n",
      "epoch: 4000, acc: 0.510, loss: 16.118, lr: 0.2000400080016003\n",
      "epoch: 4100, acc: 0.520, loss: 16.118, lr: 0.19611688566385566\n",
      "epoch: 4200, acc: 0.530, loss: 16.118, lr: 0.19234468166955185\n",
      "epoch: 4300, acc: 0.530, loss: 16.118, lr: 0.18871485185884126\n",
      "epoch: 4400, acc: 0.530, loss: 16.118, lr: 0.18521948508983144\n",
      "epoch: 4500, acc: 0.537, loss: 16.118, lr: 0.18185124568103292\n",
      "epoch: 4600, acc: 0.533, loss: 16.118, lr: 0.1786033220217896\n",
      "epoch: 4700, acc: 0.540, loss: 16.118, lr: 0.1754693805930865\n",
      "epoch: 4800, acc: 0.530, loss: 16.118, lr: 0.17244352474564578\n",
      "epoch: 4900, acc: 0.537, loss: 16.118, lr: 0.16952025767079165\n",
      "epoch: 5000, acc: 0.543, loss: 16.118, lr: 0.16669444907484582\n",
      "epoch: 5100, acc: 0.550, loss: 16.118, lr: 0.16396130513198884\n",
      "epoch: 5200, acc: 0.557, loss: 16.118, lr: 0.16131634134537828\n",
      "epoch: 5300, acc: 0.560, loss: 16.118, lr: 0.15875535799333226\n",
      "epoch: 5400, acc: 0.557, loss: 16.118, lr: 0.1562744178777934\n",
      "epoch: 5500, acc: 0.560, loss: 16.118, lr: 0.15386982612709646\n",
      "epoch: 5600, acc: 0.560, loss: 16.118, lr: 0.15153811183512653\n",
      "epoch: 5700, acc: 0.567, loss: 16.118, lr: 0.14927601134497687\n",
      "epoch: 5800, acc: 0.567, loss: 16.118, lr: 0.14708045300779526\n",
      "epoch: 5900, acc: 0.560, loss: 16.118, lr: 0.14494854326714016\n",
      "epoch: 6000, acc: 0.557, loss: 16.118, lr: 0.1428775539362766\n",
      "epoch: 6100, acc: 0.557, loss: 16.118, lr: 0.1408649105507818\n",
      "epoch: 6200, acc: 0.567, loss: 16.118, lr: 0.13890818169190167\n",
      "epoch: 6300, acc: 0.570, loss: 16.118, lr: 0.13700506918755992\n",
      "epoch: 6400, acc: 0.573, loss: 16.118, lr: 0.13515339910798757\n",
      "epoch: 6500, acc: 0.573, loss: 16.118, lr: 0.13335111348179757\n",
      "epoch: 6600, acc: 0.577, loss: 16.118, lr: 0.13159626266614027\n",
      "epoch: 6700, acc: 0.580, loss: 16.118, lr: 0.12988699831146902\n",
      "epoch: 6800, acc: 0.587, loss: 16.118, lr: 0.12822156686754713\n",
      "epoch: 6900, acc: 0.600, loss: 16.118, lr: 0.126598303582732\n",
      "epoch: 7000, acc: 0.593, loss: 16.118, lr: 0.12501562695336915\n",
      "epoch: 7100, acc: 0.593, loss: 16.118, lr: 0.12347203358439313\n",
      "epoch: 7200, acc: 0.597, loss: 16.118, lr: 0.12196609342602757\n",
      "epoch: 7300, acc: 0.593, loss: 16.118, lr: 0.12049644535486204\n",
      "epoch: 7400, acc: 0.597, loss: 16.118, lr: 0.11906179307060363\n",
      "epoch: 7500, acc: 0.607, loss: 16.118, lr: 0.11766090128250381\n",
      "epoch: 7600, acc: 0.607, loss: 16.118, lr: 0.11629259216187929\n",
      "epoch: 7700, acc: 0.617, loss: 16.118, lr: 0.11495574203931487\n",
      "epoch: 7800, acc: 0.613, loss: 16.118, lr: 0.11364927832708263\n",
      "epoch: 7900, acc: 0.623, loss: 16.118, lr: 0.11237217664906168\n",
      "epoch: 8000, acc: 0.623, loss: 16.118, lr: 0.11112345816201799\n",
      "epoch: 8100, acc: 0.630, loss: 16.118, lr: 0.10990218705352237\n",
      "epoch: 8200, acc: 0.630, loss: 16.118, lr: 0.10870746820306555\n",
      "epoch: 8300, acc: 0.637, loss: 16.118, lr: 0.1075384449940854\n",
      "epoch: 8400, acc: 0.637, loss: 16.118, lr: 0.10639429726566654\n",
      "epoch: 8500, acc: 0.640, loss: 16.118, lr: 0.10527423939362038\n",
      "epoch: 8600, acc: 0.647, loss: 16.118, lr: 0.10417751849150952\n",
      "epoch: 8700, acc: 0.657, loss: 16.118, lr: 0.10310341272296113\n",
      "epoch: 8800, acc: 0.653, loss: 16.118, lr: 0.1020512297173181\n",
      "epoch: 8900, acc: 0.657, loss: 16.118, lr: 0.10102030508132134\n",
      "epoch: 9000, acc: 0.663, loss: 16.118, lr: 0.1000100010001\n",
      "epoch: 9100, acc: 0.667, loss: 16.118, lr: 0.09901970492127933\n",
      "epoch: 9200, acc: 0.673, loss: 16.118, lr: 0.09804882831650162\n",
      "epoch: 9300, acc: 0.677, loss: 16.118, lr: 0.09709680551509856\n",
      "epoch: 9400, acc: 0.683, loss: 16.118, lr: 0.09616309260505818\n",
      "epoch: 9500, acc: 0.683, loss: 16.118, lr: 0.09524716639679968\n",
      "epoch: 9600, acc: 0.687, loss: 16.118, lr: 0.09434852344560807\n",
      "epoch: 9700, acc: 0.690, loss: 16.118, lr: 0.09346667912889055\n",
      "epoch: 9800, acc: 0.697, loss: 16.118, lr: 0.09260116677470137\n",
      "epoch: 9900, acc: 0.703, loss: 16.118, lr: 0.09175153683824203\n",
      "epoch: 10000, acc: 0.707, loss: 16.118, lr: 0.09091735612328393\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming the necessary classes (Layer_Dense, Activation_ReLU, \n",
    "# Activation_Softmax_Loss_CategoricalCrossentropy, and spiral_data) are defined elsewhere\n",
    "\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_SGD(decay=1e-3)\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "    \n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "    \n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "    \n",
    "    # Perform a forward pass through the activation/loss function\n",
    "    # takes the output of second dense layer here and returns loss\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "    \n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # calculate values along first axis\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    \n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {accuracy:.3f}, ' +\n",
    "              f'loss: {loss:.3f}, ' +\n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "    \n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>OPTIMIZERS: MOMENTUM</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Optimizer_SGD:\n",
    "    # INITIALIZE OPTIMIZER SET SETTINGS\n",
    "    # LEARNING RATE OF 1.\n",
    "    def __init__(self,learning_rate=1.0,decay=0.0,momentum=0):\n",
    "        self.learning_rate=learning_rate\n",
    "        self.current_learning_rate=learning_rate\n",
    "        self.decay=decay\n",
    "        self.iterations=0\n",
    "        self.momentum=momentum\n",
    "    # CALL ONCE BEFORE ANY PARAMETER UPDATES\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate=self.learning_rate*(1./(1.+self.decay*self.iterations))\n",
    "    # UPDATE PARAMETER\n",
    "    def update_params(self,layer):\n",
    "        # If WE USE MOMENTUM\n",
    "        if self.momentum:\n",
    "            # IF LAYER DOES NOT CONTAIN MOMENTUM ARRAY,CREATE THEM\n",
    "            # FILLED WITH ZERO\n",
    "            if not hasattr(layer,'weight_momentum'):\n",
    "                layer.weight_momentum=np.zeros_like(layer.weights)\n",
    "                layer.bias_momentum=np.zeros_like(layer.biases)\n",
    "\n",
    "            # BUILD WEIGHT UPDATES WITH MOMENTUM TAKE PREVIOUS\n",
    "            # UPDATES MULTIPLIED BY RETAIN FACTOR AND UPDATE WITH CURRENT\n",
    "\n",
    "            weight_updates=self.momentum*layer.weight_momentum-self.current_learning_rate*layer.dweights\n",
    "            layer.weight_momentum=weight_updates\n",
    "            bias_updates=self.momentum*layer.bias_momentum-self.current_learning_rate*layer.dbiases\n",
    "            layer.bias_momentum=bias_updates\n",
    "        # VANILLA SGD UPDATES\n",
    "        else:\n",
    "            weight_updates=-self.current_learning_rate*layer.dweights\n",
    "            bias_updates=-self.current_learning_rate*layer.dbiases\n",
    "\n",
    "        # UPDATE WEIGHTS AND BAISES USING EITHER VANILLA OR MOMENTUM UPDATES\n",
    "        layer.weights+=weight_updates\n",
    "        layer.biases+=bias_updates\n",
    "   # CALL ONCE AFTER PARAMETER UPDATES\n",
    "    def post_update_params(self):\n",
    "        self.iterations+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.350, loss: 16.118, lr: 1.0\n",
      "epoch: 100, acc: 0.403, loss: 16.118, lr: 0.9099181073703367\n",
      "epoch: 200, acc: 0.453, loss: 16.118, lr: 0.8340283569641367\n",
      "epoch: 300, acc: 0.553, loss: 16.118, lr: 0.7698229407236336\n",
      "epoch: 400, acc: 0.650, loss: 16.118, lr: 0.7147962830593281\n",
      "epoch: 500, acc: 0.680, loss: 16.118, lr: 0.66711140760507\n",
      "epoch: 600, acc: 0.680, loss: 16.118, lr: 0.6253908692933083\n",
      "epoch: 700, acc: 0.680, loss: 16.118, lr: 0.5885815185403178\n",
      "epoch: 800, acc: 0.697, loss: 16.118, lr: 0.5558643690939411\n",
      "epoch: 900, acc: 0.693, loss: 16.118, lr: 0.526592943654555\n",
      "epoch: 1000, acc: 0.737, loss: 16.118, lr: 0.5002501250625312\n",
      "epoch: 1100, acc: 0.737, loss: 16.118, lr: 0.4764173415912339\n",
      "epoch: 1200, acc: 0.740, loss: 16.118, lr: 0.45475216007276037\n",
      "epoch: 1300, acc: 0.737, loss: 16.118, lr: 0.43497172683775553\n",
      "epoch: 1400, acc: 0.720, loss: 16.118, lr: 0.4168403501458941\n",
      "epoch: 1500, acc: 0.723, loss: 16.118, lr: 0.4001600640256102\n",
      "epoch: 1600, acc: 0.707, loss: 16.118, lr: 0.3847633705271258\n",
      "epoch: 1700, acc: 0.707, loss: 16.118, lr: 0.3705075954057058\n",
      "epoch: 1800, acc: 0.717, loss: 16.118, lr: 0.35727045373347627\n",
      "epoch: 1900, acc: 0.723, loss: 16.118, lr: 0.3449465332873405\n",
      "epoch: 2000, acc: 0.723, loss: 16.118, lr: 0.33344448149383127\n",
      "epoch: 2100, acc: 0.723, loss: 16.118, lr: 0.32268473701193934\n",
      "epoch: 2200, acc: 0.727, loss: 16.118, lr: 0.31259768677711786\n",
      "epoch: 2300, acc: 0.723, loss: 16.118, lr: 0.3031221582297666\n",
      "epoch: 2400, acc: 0.730, loss: 16.118, lr: 0.29420417769932333\n",
      "epoch: 2500, acc: 0.730, loss: 16.118, lr: 0.2857959416976279\n",
      "epoch: 2600, acc: 0.730, loss: 16.118, lr: 0.2778549597110308\n",
      "epoch: 2700, acc: 0.737, loss: 16.118, lr: 0.2703433360367667\n",
      "epoch: 2800, acc: 0.737, loss: 16.118, lr: 0.26322716504343247\n",
      "epoch: 2900, acc: 0.737, loss: 16.118, lr: 0.25647601949217746\n",
      "epoch: 3000, acc: 0.733, loss: 16.118, lr: 0.25006251562890724\n",
      "epoch: 3100, acc: 0.733, loss: 16.118, lr: 0.2439619419370578\n",
      "epoch: 3200, acc: 0.733, loss: 16.118, lr: 0.23815194093831865\n",
      "epoch: 3300, acc: 0.737, loss: 16.118, lr: 0.23261223540358225\n",
      "epoch: 3400, acc: 0.733, loss: 16.118, lr: 0.22732439190725165\n",
      "epoch: 3500, acc: 0.733, loss: 16.118, lr: 0.22227161591464767\n",
      "epoch: 3600, acc: 0.733, loss: 16.118, lr: 0.21743857360295715\n",
      "epoch: 3700, acc: 0.737, loss: 16.118, lr: 0.21281123643328367\n",
      "epoch: 3800, acc: 0.733, loss: 16.118, lr: 0.20837674515524068\n",
      "epoch: 3900, acc: 0.737, loss: 16.118, lr: 0.20412329046744235\n",
      "epoch: 4000, acc: 0.737, loss: 16.118, lr: 0.2000400080016003\n",
      "epoch: 4100, acc: 0.723, loss: 16.118, lr: 0.19611688566385566\n",
      "epoch: 4200, acc: 0.723, loss: 16.118, lr: 0.19234468166955185\n",
      "epoch: 4300, acc: 0.723, loss: 16.118, lr: 0.18871485185884126\n",
      "epoch: 4400, acc: 0.727, loss: 16.118, lr: 0.18521948508983144\n",
      "epoch: 4500, acc: 0.730, loss: 16.118, lr: 0.18185124568103292\n",
      "epoch: 4600, acc: 0.730, loss: 16.118, lr: 0.1786033220217896\n",
      "epoch: 4700, acc: 0.733, loss: 16.118, lr: 0.1754693805930865\n",
      "epoch: 4800, acc: 0.733, loss: 16.118, lr: 0.17244352474564578\n",
      "epoch: 4900, acc: 0.733, loss: 16.118, lr: 0.16952025767079165\n",
      "epoch: 5000, acc: 0.733, loss: 16.118, lr: 0.16669444907484582\n",
      "epoch: 5100, acc: 0.730, loss: 16.118, lr: 0.16396130513198884\n",
      "epoch: 5200, acc: 0.733, loss: 16.118, lr: 0.16131634134537828\n",
      "epoch: 5300, acc: 0.733, loss: 16.118, lr: 0.15875535799333226\n",
      "epoch: 5400, acc: 0.730, loss: 16.118, lr: 0.1562744178777934\n",
      "epoch: 5500, acc: 0.730, loss: 16.118, lr: 0.15386982612709646\n",
      "epoch: 5600, acc: 0.733, loss: 16.118, lr: 0.15153811183512653\n",
      "epoch: 5700, acc: 0.730, loss: 16.118, lr: 0.14927601134497687\n",
      "epoch: 5800, acc: 0.730, loss: 16.118, lr: 0.14708045300779526\n",
      "epoch: 5900, acc: 0.733, loss: 16.118, lr: 0.14494854326714016\n",
      "epoch: 6000, acc: 0.733, loss: 16.118, lr: 0.1428775539362766\n",
      "epoch: 6100, acc: 0.737, loss: 16.118, lr: 0.1408649105507818\n",
      "epoch: 6200, acc: 0.737, loss: 16.118, lr: 0.13890818169190167\n",
      "epoch: 6300, acc: 0.737, loss: 16.118, lr: 0.13700506918755992\n",
      "epoch: 6400, acc: 0.737, loss: 16.118, lr: 0.13515339910798757\n",
      "epoch: 6500, acc: 0.737, loss: 16.118, lr: 0.13335111348179757\n",
      "epoch: 6600, acc: 0.737, loss: 16.118, lr: 0.13159626266614027\n",
      "epoch: 6700, acc: 0.737, loss: 16.118, lr: 0.12988699831146902\n",
      "epoch: 6800, acc: 0.740, loss: 16.118, lr: 0.12822156686754713\n",
      "epoch: 6900, acc: 0.740, loss: 16.118, lr: 0.126598303582732\n",
      "epoch: 7000, acc: 0.740, loss: 16.118, lr: 0.12501562695336915\n",
      "epoch: 7100, acc: 0.740, loss: 16.118, lr: 0.12347203358439313\n",
      "epoch: 7200, acc: 0.740, loss: 16.118, lr: 0.12196609342602757\n",
      "epoch: 7300, acc: 0.740, loss: 16.118, lr: 0.12049644535486204\n",
      "epoch: 7400, acc: 0.740, loss: 16.118, lr: 0.11906179307060363\n",
      "epoch: 7500, acc: 0.740, loss: 16.118, lr: 0.11766090128250381\n",
      "epoch: 7600, acc: 0.740, loss: 16.118, lr: 0.11629259216187929\n",
      "epoch: 7700, acc: 0.740, loss: 16.118, lr: 0.11495574203931487\n",
      "epoch: 7800, acc: 0.740, loss: 16.118, lr: 0.11364927832708263\n",
      "epoch: 7900, acc: 0.740, loss: 16.118, lr: 0.11237217664906168\n",
      "epoch: 8000, acc: 0.740, loss: 16.118, lr: 0.11112345816201799\n",
      "epoch: 8100, acc: 0.740, loss: 16.118, lr: 0.10990218705352237\n",
      "epoch: 8200, acc: 0.740, loss: 16.118, lr: 0.10870746820306555\n",
      "epoch: 8300, acc: 0.740, loss: 16.118, lr: 0.1075384449940854\n",
      "epoch: 8400, acc: 0.740, loss: 16.118, lr: 0.10639429726566654\n",
      "epoch: 8500, acc: 0.740, loss: 16.118, lr: 0.10527423939362038\n",
      "epoch: 8600, acc: 0.740, loss: 16.118, lr: 0.10417751849150952\n",
      "epoch: 8700, acc: 0.740, loss: 16.118, lr: 0.10310341272296113\n",
      "epoch: 8800, acc: 0.740, loss: 16.118, lr: 0.1020512297173181\n",
      "epoch: 8900, acc: 0.740, loss: 16.118, lr: 0.10102030508132134\n",
      "epoch: 9000, acc: 0.740, loss: 16.118, lr: 0.1000100010001\n",
      "epoch: 9100, acc: 0.740, loss: 16.118, lr: 0.09901970492127933\n",
      "epoch: 9200, acc: 0.740, loss: 16.118, lr: 0.09804882831650162\n",
      "epoch: 9300, acc: 0.740, loss: 16.118, lr: 0.09709680551509856\n",
      "epoch: 9400, acc: 0.737, loss: 16.118, lr: 0.09616309260505818\n",
      "epoch: 9500, acc: 0.740, loss: 16.118, lr: 0.09524716639679968\n",
      "epoch: 9600, acc: 0.740, loss: 16.118, lr: 0.09434852344560807\n",
      "epoch: 9700, acc: 0.740, loss: 16.118, lr: 0.09346667912889055\n",
      "epoch: 9800, acc: 0.740, loss: 16.118, lr: 0.09260116677470137\n",
      "epoch: 9900, acc: 0.740, loss: 16.118, lr: 0.09175153683824203\n",
      "epoch: 10000, acc: 0.740, loss: 16.118, lr: 0.09091735612328393\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming the necessary classes (Layer_Dense, Activation_ReLU, \n",
    "# Activation_Softmax_Loss_CategoricalCrossentropy, Optimizer_SGD, and spiral_data) are defined elsewhere\n",
    "\n",
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_SGD(decay=1e-3, momentum=0.9)\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "    \n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "    \n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "    \n",
    "    # Perform a forward pass through the activation/loss function\n",
    "    # takes the output of second dense layer here and returns loss\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "    \n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # calculate values along first axis\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    \n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {accuracy:.3f}, ' +\n",
    "              f'loss: {loss:.3f}, ' +\n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "    \n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>OPTIMIZERS: ADAGRAD</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADAGRAD OPTIMIZER\n",
    "class Optimizer_Adagrad:\n",
    "    # INITIALIZE OPTIMIZER SET SETTINGS\n",
    "    def __init__(self,learning_rate=1.,decay=0.0,epsilon=1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate=learning_rate\n",
    "        self.decay=decay\n",
    "        self.iterations=0\n",
    "        self.epsilon=epsilon\n",
    "\n",
    "    # CALL ONCE BEFORE ANY PARAMETER UPDATES\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate=self.learning_rate*(1./(1.+self.decay*self.iterations))\n",
    "    # UPDATE PARAMETERS\n",
    "    def update_params(self,layer):\n",
    "        # IF LAYER DOES NOT CONTAIN CACHE ARRAYS,CREATE THEM FILLED WITH ZEROS\n",
    "        if not hasattr(layer,\"weight_cache\"):\n",
    "            layer.weight_cache=np.zeros_like(layer.weights)\n",
    "            layer.bias_cache=np.zeros_like(layer.biases)\n",
    "        # UPDATE CACHE WITH SQUARED CURRENT GRADIENTS\n",
    "        layer.weight_cache+=layer.dweights**2\n",
    "        layer.bias_cache+=layer.dbiases**2\n",
    "\n",
    "        # VANILLA SGD PARAMETER UPDATE + NORMALIZATION WITH SQUARE ROOT CACHE\n",
    "        layer.weights+=-self.current_learning_rate*layer.dweights/(np.sqrt(layer.weight_cache)+self.epsilon)\n",
    "        layer.biases+=-self.current_learning_rate*layer.dbiases/(np.sqrt(layer.bias_cache)+self.epsilon)\n",
    "\n",
    "    # CALL ONCE AFTER ANY PARAMETER UPDATES\n",
    "    def post_update_params(self):\n",
    "        self.iterations+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.340, loss: 16.118, lr: 1.0\n",
      "epoch: 100, acc: 0.480, loss: 16.118, lr: 0.9901970492127933\n",
      "epoch: 200, acc: 0.490, loss: 16.118, lr: 0.9804882831650161\n",
      "epoch: 300, acc: 0.490, loss: 16.118, lr: 0.9709680551509855\n",
      "epoch: 400, acc: 0.543, loss: 16.118, lr: 0.9616309260505818\n",
      "epoch: 500, acc: 0.523, loss: 16.118, lr: 0.9524716639679969\n",
      "epoch: 600, acc: 0.553, loss: 16.118, lr: 0.9434852344560807\n",
      "epoch: 700, acc: 0.543, loss: 16.118, lr: 0.9346667912889054\n",
      "epoch: 800, acc: 0.553, loss: 16.118, lr: 0.9260116677470135\n",
      "epoch: 900, acc: 0.610, loss: 16.118, lr: 0.9175153683824203\n",
      "epoch: 1000, acc: 0.607, loss: 16.118, lr: 0.9091735612328392\n",
      "epoch: 1100, acc: 0.570, loss: 16.118, lr: 0.9009820704567978\n",
      "epoch: 1200, acc: 0.647, loss: 16.118, lr: 0.892936869363336\n",
      "epoch: 1300, acc: 0.630, loss: 16.118, lr: 0.8850340738118416\n",
      "epoch: 1400, acc: 0.663, loss: 16.118, lr: 0.8772699359592947\n",
      "epoch: 1500, acc: 0.627, loss: 16.118, lr: 0.8696408383337683\n",
      "epoch: 1600, acc: 0.680, loss: 16.118, lr: 0.8621432882145013\n",
      "epoch: 1700, acc: 0.673, loss: 16.118, lr: 0.8547739123001966\n",
      "epoch: 1800, acc: 0.660, loss: 16.118, lr: 0.8475294516484448\n",
      "epoch: 1900, acc: 0.653, loss: 16.118, lr: 0.8404067568703253\n",
      "epoch: 2000, acc: 0.710, loss: 16.118, lr: 0.8334027835652972\n",
      "epoch: 2100, acc: 0.697, loss: 16.118, lr: 0.8265145879824779\n",
      "epoch: 2200, acc: 0.693, loss: 16.118, lr: 0.8197393228953193\n",
      "epoch: 2300, acc: 0.710, loss: 16.118, lr: 0.8130742336775347\n",
      "epoch: 2400, acc: 0.740, loss: 16.118, lr: 0.8065166545689169\n",
      "epoch: 2500, acc: 0.707, loss: 16.118, lr: 0.8000640051204096\n",
      "epoch: 2600, acc: 0.753, loss: 16.118, lr: 0.7937137868084768\n",
      "epoch: 2700, acc: 0.750, loss: 16.118, lr: 0.7874635798094338\n",
      "epoch: 2800, acc: 0.723, loss: 16.118, lr: 0.7813110399249941\n",
      "epoch: 2900, acc: 0.747, loss: 16.118, lr: 0.7752538956508256\n",
      "epoch: 3000, acc: 0.760, loss: 16.118, lr: 0.7692899453804138\n",
      "epoch: 3100, acc: 0.750, loss: 16.118, lr: 0.7634170547370028\n",
      "epoch: 3200, acc: 0.763, loss: 16.118, lr: 0.7576331540268202\n",
      "epoch: 3300, acc: 0.767, loss: 16.118, lr: 0.7519362358072035\n",
      "epoch: 3400, acc: 0.760, loss: 16.118, lr: 0.7463243525636241\n",
      "epoch: 3500, acc: 0.760, loss: 16.118, lr: 0.7407956144899621\n",
      "epoch: 3600, acc: 0.780, loss: 16.118, lr: 0.735348187366718\n",
      "epoch: 3700, acc: 0.770, loss: 16.118, lr: 0.7299802905321557\n",
      "epoch: 3800, acc: 0.773, loss: 16.118, lr: 0.7246901949416624\n",
      "epoch: 3900, acc: 0.790, loss: 16.118, lr: 0.7194762213108857\n",
      "epoch: 4000, acc: 0.807, loss: 16.118, lr: 0.7143367383384527\n",
      "epoch: 4100, acc: 0.803, loss: 16.118, lr: 0.7092701610043266\n",
      "epoch: 4200, acc: 0.810, loss: 16.118, lr: 0.7042749489400663\n",
      "epoch: 4300, acc: 0.820, loss: 16.118, lr: 0.6993496048674733\n",
      "epoch: 4400, acc: 0.813, loss: 16.118, lr: 0.6944926731022988\n",
      "epoch: 4500, acc: 0.813, loss: 16.118, lr: 0.6897027381198704\n",
      "epoch: 4600, acc: 0.810, loss: 16.118, lr: 0.6849784231796698\n",
      "epoch: 4700, acc: 0.817, loss: 16.118, lr: 0.6803183890060548\n",
      "epoch: 4800, acc: 0.813, loss: 16.118, lr: 0.6757213325224677\n",
      "epoch: 4900, acc: 0.817, loss: 16.118, lr: 0.6711859856366199\n",
      "epoch: 5000, acc: 0.817, loss: 16.118, lr: 0.6667111140742716\n",
      "epoch: 5100, acc: 0.810, loss: 16.118, lr: 0.6622955162593549\n",
      "epoch: 5200, acc: 0.813, loss: 16.118, lr: 0.6579380222383051\n",
      "epoch: 5300, acc: 0.817, loss: 16.118, lr: 0.6536374926465782\n",
      "epoch: 5400, acc: 0.823, loss: 16.118, lr: 0.649392817715436\n",
      "epoch: 5500, acc: 0.823, loss: 16.118, lr: 0.6452029163171817\n",
      "epoch: 5600, acc: 0.827, loss: 16.118, lr: 0.6410667350471184\n",
      "epoch: 5700, acc: 0.827, loss: 16.118, lr: 0.6369832473405949\n",
      "epoch: 5800, acc: 0.827, loss: 16.118, lr: 0.6329514526235838\n",
      "epoch: 5900, acc: 0.823, loss: 16.118, lr: 0.6289703754953141\n",
      "epoch: 6000, acc: 0.827, loss: 16.118, lr: 0.6250390649415589\n",
      "epoch: 6100, acc: 0.823, loss: 16.118, lr: 0.6211565935772407\n",
      "epoch: 6200, acc: 0.827, loss: 16.118, lr: 0.6173220569170937\n",
      "epoch: 6300, acc: 0.827, loss: 16.118, lr: 0.6135345726731701\n",
      "epoch: 6400, acc: 0.830, loss: 16.118, lr: 0.6097932800780536\n",
      "epoch: 6500, acc: 0.827, loss: 16.118, lr: 0.6060973392326807\n",
      "epoch: 6600, acc: 0.823, loss: 16.118, lr: 0.6024459304777396\n",
      "epoch: 6700, acc: 0.827, loss: 16.118, lr: 0.5988382537876519\n",
      "epoch: 6800, acc: 0.830, loss: 16.118, lr: 0.5952735281862016\n",
      "epoch: 6900, acc: 0.830, loss: 16.118, lr: 0.5917509911829102\n",
      "epoch: 7000, acc: 0.833, loss: 16.118, lr: 0.5882698982293076\n",
      "epoch: 7100, acc: 0.823, loss: 16.118, lr: 0.5848295221942803\n",
      "epoch: 7200, acc: 0.823, loss: 16.118, lr: 0.5814291528577243\n",
      "epoch: 7300, acc: 0.823, loss: 16.118, lr: 0.5780680964217585\n",
      "epoch: 7400, acc: 0.820, loss: 16.118, lr: 0.5747456750387954\n",
      "epoch: 7500, acc: 0.813, loss: 16.118, lr: 0.5714612263557918\n",
      "epoch: 7600, acc: 0.830, loss: 16.118, lr: 0.5682141030740383\n",
      "epoch: 7700, acc: 0.837, loss: 16.118, lr: 0.5650036725238714\n",
      "epoch: 7800, acc: 0.837, loss: 16.118, lr: 0.5618293162537221\n",
      "epoch: 7900, acc: 0.840, loss: 16.118, lr: 0.5586904296329404\n",
      "epoch: 8000, acc: 0.837, loss: 16.118, lr: 0.5555864214678593\n",
      "epoch: 8100, acc: 0.820, loss: 16.118, lr: 0.5525167136305873\n",
      "epoch: 8200, acc: 0.817, loss: 16.118, lr: 0.5494807407000385\n",
      "epoch: 8300, acc: 0.823, loss: 16.118, lr: 0.5464779496147331\n",
      "epoch: 8400, acc: 0.817, loss: 16.118, lr: 0.5435077993369205\n",
      "epoch: 8500, acc: 0.810, loss: 16.118, lr: 0.5405697605275961\n",
      "epoch: 8600, acc: 0.810, loss: 16.118, lr: 0.5376633152320017\n",
      "epoch: 8700, acc: 0.813, loss: 16.118, lr: 0.5347879565752179\n",
      "epoch: 8800, acc: 0.813, loss: 16.118, lr: 0.5319431884674717\n",
      "epoch: 8900, acc: 0.810, loss: 16.118, lr: 0.5291285253188\n",
      "epoch: 9000, acc: 0.813, loss: 16.118, lr: 0.5263434917627243\n",
      "epoch: 9100, acc: 0.813, loss: 16.118, lr: 0.5235876223886068\n",
      "epoch: 9200, acc: 0.813, loss: 16.118, lr: 0.5208604614823689\n",
      "epoch: 9300, acc: 0.813, loss: 16.118, lr: 0.5181615627752734\n",
      "epoch: 9400, acc: 0.810, loss: 16.118, lr: 0.5154904892004742\n",
      "epoch: 9500, acc: 0.810, loss: 16.118, lr: 0.5128468126570593\n",
      "epoch: 9600, acc: 0.813, loss: 16.118, lr: 0.5102301137813153\n",
      "epoch: 9700, acc: 0.813, loss: 16.118, lr: 0.5076399817249606\n",
      "epoch: 9800, acc: 0.813, loss: 16.118, lr: 0.5050760139400979\n",
      "epoch: 9900, acc: 0.813, loss: 16.118, lr: 0.5025378159706518\n",
      "epoch: 10000, acc: 0.813, loss: 16.118, lr: 0.5000250012500626\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 64 input features (as we take output of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Create optimizer\n",
    "# optimizer = Optimizer_SGD(decay=8e-8, momentum=0.9)\n",
    "optimizer = Optimizer_Adagrad(decay=1e-4)\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "    \n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "    \n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "    \n",
    "    # Perform a forward pass through the activation/loss function\n",
    "    # takes the output of second dense layer here and returns loss\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "    \n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # calculate values along first axis\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    \n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {accuracy:.3f}, ' +\n",
    "              f'loss: {loss:.3f}, ' +\n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "    \n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>OPTIMIZERS: RMS PROP</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Optimizer_RMSprop:\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, rho=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.rho * layer.weight_cache + \\\n",
    "                             (1 - self.rho) * layer.dweights**2\n",
    "        layer.bias_cache = self.rho * layer.bias_cache + \\\n",
    "                           (1 - self.rho) * layer.dbiases**2\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                         layer.dweights / \\\n",
    "                         (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                        layer.dbiases / \\\n",
    "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.333, loss: 16.118, lr: 0.02\n",
      "epoch: 100, acc: 0.410, loss: 16.118, lr: 0.01998021958261321\n",
      "epoch: 200, acc: 0.500, loss: 16.118, lr: 0.019960279044701046\n",
      "epoch: 300, acc: 0.550, loss: 16.118, lr: 0.019940378268975763\n",
      "epoch: 400, acc: 0.577, loss: 16.118, lr: 0.01992051713662487\n",
      "epoch: 500, acc: 0.623, loss: 16.118, lr: 0.01990069552930875\n",
      "epoch: 600, acc: 0.663, loss: 16.118, lr: 0.019880913329158343\n",
      "epoch: 700, acc: 0.650, loss: 16.118, lr: 0.019861170418772778\n",
      "epoch: 800, acc: 0.680, loss: 16.118, lr: 0.019841466681217078\n",
      "epoch: 900, acc: 0.670, loss: 16.118, lr: 0.01982180200001982\n",
      "epoch: 1000, acc: 0.643, loss: 16.118, lr: 0.019802176259170884\n",
      "epoch: 1100, acc: 0.670, loss: 16.118, lr: 0.01978258934311912\n",
      "epoch: 1200, acc: 0.673, loss: 16.118, lr: 0.01976304113677013\n",
      "epoch: 1300, acc: 0.720, loss: 16.118, lr: 0.019743531525483964\n",
      "epoch: 1400, acc: 0.690, loss: 16.118, lr: 0.01972406039507293\n",
      "epoch: 1500, acc: 0.713, loss: 16.118, lr: 0.019704627631799327\n",
      "epoch: 1600, acc: 0.733, loss: 16.118, lr: 0.019685233122373254\n",
      "epoch: 1700, acc: 0.740, loss: 16.118, lr: 0.019665876753950384\n",
      "epoch: 1800, acc: 0.737, loss: 16.118, lr: 0.01964655841412981\n",
      "epoch: 1900, acc: 0.747, loss: 16.118, lr: 0.019627277990951823\n",
      "epoch: 2000, acc: 0.763, loss: 16.118, lr: 0.019608035372895814\n",
      "epoch: 2100, acc: 0.787, loss: 16.118, lr: 0.01958883044887805\n",
      "epoch: 2200, acc: 0.783, loss: 16.118, lr: 0.019569663108249594\n",
      "epoch: 2300, acc: 0.770, loss: 16.118, lr: 0.01955053324079414\n",
      "epoch: 2400, acc: 0.800, loss: 16.118, lr: 0.019531440736725945\n",
      "epoch: 2500, acc: 0.807, loss: 16.118, lr: 0.019512385486687673\n",
      "epoch: 2600, acc: 0.770, loss: 16.118, lr: 0.019493367381748363\n",
      "epoch: 2700, acc: 0.753, loss: 16.118, lr: 0.019474386313401298\n",
      "epoch: 2800, acc: 0.760, loss: 16.118, lr: 0.019455442173562\n",
      "epoch: 2900, acc: 0.767, loss: 16.118, lr: 0.019436534854566128\n",
      "epoch: 3000, acc: 0.780, loss: 16.118, lr: 0.01941766424916747\n",
      "epoch: 3100, acc: 0.767, loss: 16.118, lr: 0.019398830250535893\n",
      "epoch: 3200, acc: 0.783, loss: 16.118, lr: 0.019380032752255354\n",
      "epoch: 3300, acc: 0.787, loss: 16.118, lr: 0.01936127164832186\n",
      "epoch: 3400, acc: 0.790, loss: 16.118, lr: 0.01934254683314152\n",
      "epoch: 3500, acc: 0.860, loss: 16.118, lr: 0.019323858201528515\n",
      "epoch: 3600, acc: 0.837, loss: 16.118, lr: 0.019305205648703173\n",
      "epoch: 3700, acc: 0.850, loss: 16.118, lr: 0.01928658907028997\n",
      "epoch: 3800, acc: 0.867, loss: 16.118, lr: 0.01926800836231563\n",
      "epoch: 3900, acc: 0.857, loss: 16.118, lr: 0.019249463421207133\n",
      "epoch: 4000, acc: 0.837, loss: 16.118, lr: 0.019230954143789846\n",
      "epoch: 4100, acc: 0.827, loss: 16.118, lr: 0.019212480427285565\n",
      "epoch: 4200, acc: 0.837, loss: 16.118, lr: 0.019194042169310647\n",
      "epoch: 4300, acc: 0.837, loss: 16.118, lr: 0.019175639267874092\n",
      "epoch: 4400, acc: 0.690, loss: 16.118, lr: 0.019157271621375684\n",
      "epoch: 4500, acc: 0.877, loss: 16.118, lr: 0.0191389391286041\n",
      "epoch: 4600, acc: 0.867, loss: 16.118, lr: 0.019120641688735073\n",
      "epoch: 4700, acc: 0.860, loss: 16.118, lr: 0.019102379201329525\n",
      "epoch: 4800, acc: 0.853, loss: 16.118, lr: 0.01908415156633174\n",
      "epoch: 4900, acc: 0.877, loss: 16.118, lr: 0.01906595868406753\n",
      "epoch: 5000, acc: 0.700, loss: 16.118, lr: 0.01904780045524243\n",
      "epoch: 5100, acc: 0.897, loss: 16.118, lr: 0.019029676780939874\n",
      "epoch: 5200, acc: 0.890, loss: 16.118, lr: 0.019011587562619416\n",
      "epoch: 5300, acc: 0.890, loss: 16.118, lr: 0.01899353270211493\n",
      "epoch: 5400, acc: 0.890, loss: 16.118, lr: 0.018975512101632844\n",
      "epoch: 5500, acc: 0.897, loss: 16.118, lr: 0.018957525663750367\n",
      "epoch: 5600, acc: 0.893, loss: 16.118, lr: 0.018939573291413745\n",
      "epoch: 5700, acc: 0.897, loss: 16.118, lr: 0.018921654887936498\n",
      "epoch: 5800, acc: 0.913, loss: 16.118, lr: 0.018903770356997706\n",
      "epoch: 5900, acc: 0.870, loss: 16.118, lr: 0.018885919602640248\n",
      "epoch: 6000, acc: 0.890, loss: 16.118, lr: 0.018868102529269144\n",
      "epoch: 6100, acc: 0.903, loss: 16.118, lr: 0.018850319041649778\n",
      "epoch: 6200, acc: 0.913, loss: 16.118, lr: 0.018832569044906263\n",
      "epoch: 6300, acc: 0.927, loss: 16.118, lr: 0.018814852444519702\n",
      "epoch: 6400, acc: 0.927, loss: 16.118, lr: 0.018797169146326564\n",
      "epoch: 6500, acc: 0.903, loss: 16.118, lr: 0.01877951905651696\n",
      "epoch: 6600, acc: 0.887, loss: 16.118, lr: 0.018761902081633034\n",
      "epoch: 6700, acc: 0.930, loss: 16.118, lr: 0.018744318128567278\n",
      "epoch: 6800, acc: 0.893, loss: 16.118, lr: 0.018726767104560903\n",
      "epoch: 6900, acc: 0.890, loss: 16.118, lr: 0.018709248917202218\n",
      "epoch: 7000, acc: 0.923, loss: 16.118, lr: 0.018691763474424996\n",
      "epoch: 7100, acc: 0.930, loss: 16.118, lr: 0.018674310684506857\n",
      "epoch: 7200, acc: 0.897, loss: 16.118, lr: 0.01865689045606769\n",
      "epoch: 7300, acc: 0.910, loss: 16.118, lr: 0.01863950269806802\n",
      "epoch: 7400, acc: 0.907, loss: 16.118, lr: 0.018622147319807447\n",
      "epoch: 7500, acc: 0.910, loss: 16.118, lr: 0.018604824230923075\n",
      "epoch: 7600, acc: 0.930, loss: 16.118, lr: 0.01858753334138793\n",
      "epoch: 7700, acc: 0.917, loss: 16.118, lr: 0.018570274561509396\n",
      "epoch: 7800, acc: 0.933, loss: 16.118, lr: 0.018553047801927663\n",
      "epoch: 7900, acc: 0.907, loss: 16.118, lr: 0.018535852973614212\n",
      "epoch: 8000, acc: 0.937, loss: 16.118, lr: 0.01851868998787026\n",
      "epoch: 8100, acc: 0.900, loss: 16.118, lr: 0.018501558756325222\n",
      "epoch: 8200, acc: 0.900, loss: 16.118, lr: 0.01848445919093522\n",
      "epoch: 8300, acc: 0.900, loss: 16.118, lr: 0.018467391203981567\n",
      "epoch: 8400, acc: 0.900, loss: 16.118, lr: 0.018450354708069265\n",
      "epoch: 8500, acc: 0.907, loss: 16.118, lr: 0.018433349616125496\n",
      "epoch: 8600, acc: 0.940, loss: 16.118, lr: 0.018416375841398172\n",
      "epoch: 8700, acc: 0.897, loss: 16.118, lr: 0.01839943329745444\n",
      "epoch: 8800, acc: 0.903, loss: 16.118, lr: 0.01838252189817921\n",
      "epoch: 8900, acc: 0.903, loss: 16.118, lr: 0.018365641557773718\n",
      "epoch: 9000, acc: 0.903, loss: 16.118, lr: 0.018348792190754044\n",
      "epoch: 9100, acc: 0.930, loss: 16.118, lr: 0.0183319737119497\n",
      "epoch: 9200, acc: 0.943, loss: 16.118, lr: 0.018315186036502167\n",
      "epoch: 9300, acc: 0.933, loss: 16.118, lr: 0.018298429079863496\n",
      "epoch: 9400, acc: 0.940, loss: 16.118, lr: 0.018281702757794862\n",
      "epoch: 9500, acc: 0.907, loss: 16.118, lr: 0.018265006986365174\n",
      "epoch: 9600, acc: 0.903, loss: 16.118, lr: 0.018248341681949654\n",
      "epoch: 9700, acc: 0.903, loss: 16.118, lr: 0.018231706761228456\n",
      "epoch: 9800, acc: 0.903, loss: 16.118, lr: 0.018215102141185255\n",
      "epoch: 9900, acc: 0.903, loss: 16.118, lr: 0.018198527739105907\n",
      "epoch: 10000, acc: 0.940, loss: 16.118, lr: 0.018181983472577025\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 64 input features (as we take output of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_RMSprop(decay=1e-5,learning_rate=0.02,rho=0.999)\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "    \n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "    \n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "    \n",
    "    # Perform a forward pass through the activation/loss function\n",
    "    # takes the output of second dense layer here and returns loss\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "    \n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # calculate values along first axis\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    \n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {accuracy:.3f}, ' +\n",
    "              f'loss: {loss:.3f}, ' +\n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "    \n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
